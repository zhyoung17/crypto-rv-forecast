{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-02 01:00:00+00:00\n",
      "2024-04-02 01:00:00+00:00\n",
      "2024-04-02 01:00:00+00:00\n",
      "2024-04-02 01:00:00+00:00\n",
      "2024-04-02 01:00:00+00:00\n",
      "2024-04-02 01:00:00+00:00\n",
      "                        Date    Ticker Risk Group Frequency     Actual  \\\n",
      "0  2024-04-02 01:00:00+00:00   BTC-USD        low    hourly -14.731732   \n",
      "1  2024-04-02 01:00:00+00:00  DOGE-USD       high    hourly -10.180126   \n",
      "2  2024-04-02 01:00:00+00:00   ETH-USD        low    hourly -13.544106   \n",
      "3  2024-04-02 01:00:00+00:00   SOL-USD       high    hourly -13.904746   \n",
      "4  2024-04-02 01:00:00+00:00   XRP-USD     medium    hourly  -9.037092   \n",
      "\n",
      "   bagging_dt        har      garch         rf        svr       ewma  \n",
      "0  -13.012559 -12.717092 -12.434889 -13.095386 -12.625901 -12.417476  \n",
      "1  -11.402712 -11.096927 -10.314513 -11.573167 -10.983400 -10.155544  \n",
      "2  -12.694841 -13.315415 -12.036654 -12.615214 -12.464753 -12.092227  \n",
      "3  -11.667074 -11.123150 -11.039991 -11.701376 -11.148741 -11.010307  \n",
      "4  -12.689666 -12.132799 -11.660224 -12.696592 -12.257942 -12.226351  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load forecast data from CSV files\n",
    "bagging_dt = pd.read_csv('../../results/bagging_dt.csv')\n",
    "har = pd.read_csv('../../results/har.csv')\n",
    "garch = pd.read_csv('../../results/garch.csv')\n",
    "rf = pd.read_csv('../../results/rf.csv')\n",
    "svr = pd.read_csv('../../results/svr.csv')\n",
    "ewma = pd.read_csv('../../results/ewma.csv')\n",
    "\n",
    "# Rename the \"Predicted\" column to the model name for each DataFrame\n",
    "bagging_dt = bagging_dt.rename(columns={'Predicted': 'bagging_dt'})\n",
    "har = har.rename(columns={'Predicted': 'har'})\n",
    "garch = garch.rename(columns={'Predicted': 'garch'})\n",
    "rf = rf.rename(columns={'Predicted': 'rf'})\n",
    "svr = svr.rename(columns={'Predicted': 'svr'})\n",
    "ewma = ewma.rename(columns={'Predicted': 'ewma'})\n",
    "\n",
    "# Some of the risk group names are not standardised (Caps, lower case, etc.)\n",
    "# We need to standardise them before merging the DataFrames\n",
    "bagging_dt['Risk Group'] = bagging_dt['Risk Group'].str.lower()\n",
    "har['Risk Group'] = har['Risk Group'].str.lower()\n",
    "garch['Risk Group'] = garch['Risk Group'].str.lower()\n",
    "rf['Risk Group'] = rf['Risk Group'].str.lower()\n",
    "svr['Risk Group'] = svr['Risk Group'].str.lower()\n",
    "ewma['Risk Group'] = ewma['Risk Group'].str.lower()\n",
    "\n",
    "\n",
    "# Print the first date of all models for checking\n",
    "print(har['Date'].iloc[0])\n",
    "print(garch['Date'].iloc[0])\n",
    "print(rf['Date'].iloc[0])\n",
    "print(svr['Date'].iloc[0])\n",
    "print(ewma['Date'].iloc[0])\n",
    "print(bagging_dt['Date'].iloc[0])\n",
    "\n",
    "# Start with one DataFrame and merge the others on the common columns\n",
    "data = bagging_dt[['Date', 'Ticker', 'Risk Group', 'Frequency', 'Actual', 'bagging_dt']]\n",
    "\n",
    "data = data.merge(har[['Date', 'Ticker', 'Risk Group', 'Frequency', 'har']],\n",
    "                  on=['Date', 'Ticker', 'Risk Group', 'Frequency'], how='outer')\n",
    "data = data.merge(garch[['Date', 'Ticker', 'Risk Group', 'Frequency', 'garch']],\n",
    "                  on=['Date', 'Ticker', 'Risk Group', 'Frequency'], how='outer')\n",
    "data = data.merge(rf[['Date', 'Ticker', 'Risk Group', 'Frequency', 'rf']],\n",
    "                  on=['Date', 'Ticker', 'Risk Group', 'Frequency'], how='outer')\n",
    "data = data.merge(svr[['Date', 'Ticker', 'Risk Group', 'Frequency', 'svr']],\n",
    "                  on=['Date', 'Ticker', 'Risk Group', 'Frequency'], how='outer')\n",
    "data = data.merge(ewma[['Date', 'Ticker', 'Risk Group', 'Frequency', 'ewma']],\n",
    "                  on=['Date', 'Ticker', 'Risk Group', 'Frequency'], how='outer')\n",
    "\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum and maximum RMSE:\n",
      "                 min       max\n",
      "Model                         \n",
      "bagging_dt  0.859443  2.485639\n",
      "ewma        0.885295  2.553039\n",
      "garch       0.884789  2.443261\n",
      "har         0.897821  2.573997\n",
      "rf          0.867560  2.488804\n",
      "svr         0.851058  2.471022\n",
      "Minimum and Maximum R2:\n",
      "                 min       max\n",
      "Model                         \n",
      "bagging_dt  0.019736  0.388133\n",
      "ewma       -0.059974  0.368939\n",
      "garch       0.046368  0.327220\n",
      "har        -0.007979  0.318631\n",
      "rf          0.017238  0.387768\n",
      "svr         0.023019  0.310761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = mean_absolute_percentage_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    qlike = np.mean(actual / predicted - np.log(actual / predicted) - 1)\n",
    "    return mse, rmse, mape, r2, qlike\n",
    "\n",
    "# List of model names\n",
    "model_names = ['bagging_dt', 'har', 'garch', 'rf', 'svr', 'ewma']\n",
    "\n",
    "# Create a list to hold metric rows.\n",
    "rows = []\n",
    "\n",
    "# Group the data by Risk Group and Frequency.\n",
    "grouped = data.groupby(['Risk Group', 'Frequency'])\n",
    "\n",
    "for (risk, freq), group in grouped:\n",
    "    # For each model, compute evaluation metrics within the group.\n",
    "    for model in model_names:\n",
    "        mse, rmse, mape, r2, qlike = calculate_metrics(group['Actual'], group[model])\n",
    "        rows.append({\n",
    "            'Risk Group': risk,\n",
    "            'Frequency': freq,\n",
    "            'Model': model,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R2': r2,\n",
    "            'QLIKE': qlike\n",
    "        })\n",
    "\n",
    "# Convert the list of rows into a DataFrame.\n",
    "grouped_metrics_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Minimum and maximum RMSE:\")\n",
    "print(grouped_metrics_df.groupby(['Model'])['RMSE'].agg(['min', 'max']))\n",
    "print(\"Minimum and Maximum R2:\")\n",
    "print(grouped_metrics_df.groupby(['Model'])['R2'].agg(['min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract EWMA RMSE for each Risk Group and Frequency\n",
    "ewma_rmse = grouped_metrics_df[grouped_metrics_df['Model'] == 'ewma'][['Frequency','Risk Group', 'RMSE']]\n",
    "ewma_rmse = ewma_rmse.rename(columns={'RMSE': 'ewma_RMSE'}).set_index(['Frequency','Risk Group'])\n",
    "\n",
    "# Merge the EWMA RMSE back into the original DataFrame using the keys\n",
    "grouped_metrics_df = grouped_metrics_df.merge(ewma_rmse, left_on=['Frequency','Risk Group'], right_index=True)\n",
    "\n",
    "# Calculate the Relative RMSE by dividing RMSE by the benchmark ewma_RMSE\n",
    "grouped_metrics_df['Relative RMSE'] = grouped_metrics_df['RMSE'] / grouped_metrics_df['ewma_RMSE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_6632\\1709267665.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  summary['Risk Group'] = pd.Categorical(summary['Risk Group'], categories=['low', 'medium', 'high'], ordered=True)\n"
     ]
    }
   ],
   "source": [
    "# Order the Summary table: Risk Group, Frequency, Model, Relative RMSE, R2, MAPE, QLIKE\n",
    "summary = grouped_metrics_df[['Frequency','Risk Group', 'Model', 'Relative RMSE', 'R2', 'MAPE', 'QLIKE']]\n",
    "\n",
    "# Order the risk groups to be low, medium, high\n",
    "summary['Risk Group'] = pd.Categorical(summary['Risk Group'], categories=['low', 'medium', 'high'], ordered=True)\n",
    "summary = summary.sort_values(['Frequency', 'Risk Group', 'Relative RMSE'])\n",
    "\n",
    "# Order the frequ to be hourly, 3hourly, daily\n",
    "summary['Frequency'] = pd.Categorical(summary['Frequency'], categories=['hourly', '3hourly', 'daily'], ordered=True)\n",
    "summary = summary.sort_values(['Frequency', 'Risk Group', 'Relative RMSE'])\n",
    "\n",
    "# Sort it by Risk Group, Frequency, and Relative RMSE (from highest to lowest)\n",
    "summary = summary.sort_values(['Frequency', 'Risk Group', 'Relative RMSE'], ascending=[True, True, False])\n",
    "summary\n",
    "\n",
    "summary.to_csv('../../results/evaluation/summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Models\n",
    "This study will now identify the best models for each frequency and risk group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2\n",
    "In some papers, the lowest RMSE does not represent the highest R2. However in this paper, the lowest RMSE corresponded with the highest RMSE. So in this component, we will extract the highest R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_6632\\2546037691.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  best_models_r2 = summary.groupby(['Frequency','Risk Group']).first()\n"
     ]
    }
   ],
   "source": [
    "summary = summary.sort_values(['Frequency', 'Risk Group', 'R2'], ascending=[True, True, False])\n",
    "best_models_r2 = summary.groupby(['Frequency','Risk Group']).first()\n",
    "best_models_r2\n",
    "best_models_r2.to_csv('../../results/evaluation/best_models_r2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diebold-Mariano test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Risk Group Frequency       Model  DM Statistic       p-value  \\\n",
      "0        high   3hourly  bagging_dt      4.069463  4.778955e-05   \n",
      "1        high   3hourly         har      7.053845  1.957712e-12   \n",
      "2        high   3hourly       garch      6.413745  1.539905e-10   \n",
      "3        high   3hourly          rf      4.423390  9.905995e-06   \n",
      "4        high   3hourly         svr      6.097852  1.149130e-09   \n",
      "5        high     daily  bagging_dt     -1.525301  1.276819e-01   \n",
      "6        high     daily         har      0.980475  3.272251e-01   \n",
      "7        high     daily       garch     -0.068204  9.456446e-01   \n",
      "8        high     daily          rf     -0.953183  3.408598e-01   \n",
      "9        high     daily         svr     -1.774776  7.641395e-02   \n",
      "10       high    hourly  bagging_dt    -10.598774  3.668274e-26   \n",
      "11       high    hourly         har     -6.645093  3.125613e-11   \n",
      "12       high    hourly       garch    -17.788339  3.960827e-70   \n",
      "13       high    hourly          rf    -10.223755  1.837676e-24   \n",
      "14       high    hourly         svr    -11.031527  3.382634e-28   \n",
      "15        low   3hourly  bagging_dt      2.351010  1.875806e-02   \n",
      "16        low   3hourly         har      5.021534  5.289685e-07   \n",
      "17        low   3hourly       garch     10.513659  1.314074e-25   \n",
      "18        low   3hourly          rf      2.675506  7.484017e-03   \n",
      "19        low   3hourly         svr      3.228340  1.252504e-03   \n",
      "20        low     daily  bagging_dt     -2.529339  1.166863e-02   \n",
      "21        low     daily         har      1.228314  2.197841e-01   \n",
      "22        low     daily       garch     -0.416668  6.770624e-01   \n",
      "23        low     daily          rf     -2.096301  3.645063e-02   \n",
      "24        low     daily         svr      1.358968  1.746392e-01   \n",
      "25        low    hourly  bagging_dt      5.088897  3.641297e-07   \n",
      "26        low    hourly         har      9.313394  1.391558e-20   \n",
      "27        low    hourly       garch     -3.219059  1.288622e-03   \n",
      "28        low    hourly          rf      5.475979  4.414907e-08   \n",
      "29        low    hourly         svr      1.496770  1.344724e-01   \n",
      "30     medium   3hourly  bagging_dt      5.538479  3.343244e-08   \n",
      "31     medium   3hourly         har      5.207366  2.058564e-07   \n",
      "32     medium   3hourly       garch      7.272300  4.599784e-13   \n",
      "33     medium   3hourly          rf      5.331411  1.054560e-07   \n",
      "34     medium   3hourly         svr      5.924441  3.528502e-09   \n",
      "35     medium     daily  bagging_dt     -1.569445  1.175419e-01   \n",
      "36     medium     daily         har      0.169086  8.658368e-01   \n",
      "37     medium     daily       garch      2.188062  2.939476e-02   \n",
      "38     medium     daily          rf     -1.513999  1.310225e-01   \n",
      "39     medium     daily         svr      0.321477  7.480611e-01   \n",
      "40     medium    hourly  bagging_dt      2.219275  2.649526e-02   \n",
      "41     medium    hourly         har      8.789533  1.807720e-18   \n",
      "42     medium    hourly       garch     -6.443132  1.236293e-10   \n",
      "43     medium    hourly          rf      2.404734  1.620627e-02   \n",
      "44     medium    hourly         svr      1.054516  2.916776e-01   \n",
      "\n",
      "                    Recommendation  \n",
      "0        EWMA model should be used  \n",
      "1        EWMA model should be used  \n",
      "2        EWMA model should be used  \n",
      "3        EWMA model should be used  \n",
      "4        EWMA model should be used  \n",
      "5        No significant difference  \n",
      "6        No significant difference  \n",
      "7        No significant difference  \n",
      "8        No significant difference  \n",
      "9        No significant difference  \n",
      "10  Alternate model should be used  \n",
      "11  Alternate model should be used  \n",
      "12  Alternate model should be used  \n",
      "13  Alternate model should be used  \n",
      "14  Alternate model should be used  \n",
      "15       EWMA model should be used  \n",
      "16       EWMA model should be used  \n",
      "17       EWMA model should be used  \n",
      "18       EWMA model should be used  \n",
      "19       EWMA model should be used  \n",
      "20  Alternate model should be used  \n",
      "21       No significant difference  \n",
      "22       No significant difference  \n",
      "23  Alternate model should be used  \n",
      "24       No significant difference  \n",
      "25       EWMA model should be used  \n",
      "26       EWMA model should be used  \n",
      "27  Alternate model should be used  \n",
      "28       EWMA model should be used  \n",
      "29       No significant difference  \n",
      "30       EWMA model should be used  \n",
      "31       EWMA model should be used  \n",
      "32       EWMA model should be used  \n",
      "33       EWMA model should be used  \n",
      "34       EWMA model should be used  \n",
      "35       No significant difference  \n",
      "36       No significant difference  \n",
      "37       EWMA model should be used  \n",
      "38       No significant difference  \n",
      "39       No significant difference  \n",
      "40       EWMA model should be used  \n",
      "41       EWMA model should be used  \n",
      "42  Alternate model should be used  \n",
      "43       EWMA model should be used  \n",
      "44       No significant difference  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')  # Ensure the module is in the path\n",
    "from dm_test import dm_test  # Import the DM test function\n",
    "import pandas as pd\n",
    "\n",
    "# List of alternative models to compare (excluding the benchmark 'ewma')\n",
    "models = ['bagging_dt', 'har', 'garch', 'rf', 'svr']\n",
    "\n",
    "# Initialize a list to store DM test results\n",
    "dm_results = []\n",
    "\n",
    "# Group the DataFrame by 'Risk Group' and 'Frequency'\n",
    "for (risk_group, freq), group in data.groupby(['Risk Group', 'Frequency']):\n",
    "    # Extract actual values and the EWMA forecast as lists\n",
    "    actual_lst = group['Actual'].tolist()\n",
    "    ewma_lst = group['ewma'].tolist()\n",
    "    \n",
    "    # Loop over each alternative model and perform the DM test comparing its forecast against EWMA\n",
    "    for model in models:\n",
    "        model_pred_lst = group[model].tolist()\n",
    "        try:\n",
    "            # Run the DM test using h=1 and the \"MSE\" criterion\n",
    "            dm_result = dm_test(actual_lst, model_pred_lst, ewma_lst, h=1, crit=\"MSE\", power=2)\n",
    "            \n",
    "            # Determine recommendation based on significance and sign of DM statistic\n",
    "            if dm_result.p_value < 0.05:\n",
    "                if dm_result.DM < 0:\n",
    "                    recommendation = \"Alternate model should be used\"\n",
    "                else:\n",
    "                    recommendation = \"EWMA model should be used\"\n",
    "            else:\n",
    "                recommendation = \"No significant difference\"\n",
    "            \n",
    "            # Append the results along with recommendation\n",
    "            dm_results.append({\n",
    "                'Risk Group': risk_group,\n",
    "                'Frequency': freq,\n",
    "                'Model': model,\n",
    "                'DM Statistic': dm_result.DM,\n",
    "                'p-value': dm_result.p_value,\n",
    "                'Recommendation': recommendation\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error for group {risk_group} {freq}, model {model}: {e}\")\n",
    "\n",
    "# Convert the results into a DataFrame and display it\n",
    "dm_results_df = pd.DataFrame(dm_results)\n",
    "print(dm_results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
