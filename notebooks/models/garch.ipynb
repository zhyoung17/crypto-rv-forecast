{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR Model\n",
    "For each risk classficiation, we will train a model to fit to predict the RV model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries and data\n",
    "To obtain the data, please go to notebooks/data_preprocessing, and then run data_import.ipynb and then run data_preprocessing.ipynb. This will give you data/processed_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker',\n",
      "       'ln_hourly_return', 'ln_3_hourly_return', 'hourly_rv', '3_hourly_rv',\n",
      "       'ln_hourly_rv', 'ln_3_hourly_rv', 'ln_daily_rv', 'weekly_rv',\n",
      "       'ln_weekly_rv', 'monthly_rv', 'ln_monthly_rv', 'ln_daily_rv_lag1',\n",
      "       'ln_daily_rv_lag2', 'ln_weekly_rv_lag1', 'ln_weekly_rv_lag2',\n",
      "       'ln_monthly_rv_lag1', 'ln_monthly_rv_lag2', 'ln_hourly_rv_lag1',\n",
      "       'ln_3_hourly_rv_lag1', 'ln_hourly_rv_lag2', 'ln_3_hourly_rv_lag2',\n",
      "       'ln_hourly_return_lag1', 'ln_3_hourly_return_lag1',\n",
      "       'ln_hourly_return_lag2', 'ln_3_hourly_return_lag2', 'hourly_rv_lag1',\n",
      "       'hourly_rv_lag2', 'three_hourly_rv_lag1', 'three_hourly_rv_lag2',\n",
      "       'daily_rv', 'Risk'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from arch import arch_model\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../../data/processed_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Remove NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Print columns\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "Now we will use a different train-test split from the group project\n",
    "Group project: 80/20 split\n",
    "Individual: Use 1 year of training data, then use rolling window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 00:00:00+00:00 2024-05-03 00:00:00+00:00\n",
      "2024-05-03 01:00:00+00:00 2025-03-10 23:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "# Sort the data by date\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Determine when the first year ends, and use it as train data\n",
    "# The rest of the data is used as test data\n",
    "min_date = data['Date'].min()\n",
    "max_date = data['Date'].max()\n",
    "\n",
    "# Calculate the total time span of the data\n",
    "total_time_span = max_date - min_date\n",
    "\n",
    "# Define the first year of data\n",
    "first_year_end = min_date + pd.DateOffset(years=1)\n",
    "\n",
    "# Filter data for the first year\n",
    "first_year_data = data[data['Date'] <= first_year_end]\n",
    "\n",
    "# Calculate the percentage of data in the first year\n",
    "percentage_first_year = (len(first_year_data) / len(data))\n",
    "\n",
    "train_split = percentage_first_year\n",
    "train_data = data[:int(train_split * len(data))]\n",
    "test_data = data[int(train_split * len(data)):]\n",
    "\n",
    "# Print train and test data date\n",
    "print(train_data['Date'].min(), train_data['Date'].max())\n",
    "print(test_data['Date'].min(), test_data['Date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further split the data based on the risk level\n",
    "There are low, medium, and high risk models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split train and test data by risk groups\n",
    "train_data_split = {\n",
    "    'low': train_data[train_data['Risk'] == 'Low Risk'],\n",
    "    'medium': train_data[train_data['Risk'] == 'Medium Risk'],\n",
    "    'high': train_data[train_data['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_data_split = {\n",
    "    'low': test_data[test_data['Risk'] == 'Low Risk'],\n",
    "    'medium': test_data[test_data['Risk'] == 'Medium Risk'],\n",
    "    'high': test_data[test_data['Risk'] == 'High Risk']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model based on their classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wil give us 3 models to work with: model_low, model_medium, and model_high. We will use these subsequent models on the test data to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_4176\\628898507.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, pd.DataFrame([row])], ignore_index=True)\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\arch\\univariate\\base.py:768: ConvergenceWarning: The optimizer returned code 4. The message is:\n",
      "Inequality constraints incompatible\n",
      "See scipy.optimize.fmin_slsqp for code meaning.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def train_models_by_frequency_and_risk(train_data, frequencies):\n",
    "    models = {}\n",
    "    model_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Omega', 'Alpha[1]', 'Beta[1]', 'Mean'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        models[freq] = {}\n",
    "        # Select target column based on frequency\n",
    "        if freq == 'hourly':\n",
    "            target_col = 'hourly_rv'\n",
    "        elif freq == '3hourly':\n",
    "            target_col = '3_hourly_rv'\n",
    "        elif freq == 'daily':\n",
    "            target_col = 'daily_rv'\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported frequency: {freq}\")\n",
    "            \n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            df_train = train_data[risk_group].copy()\n",
    "            if target_col not in df_train.columns:\n",
    "                print(f\"Missing {target_col} for {freq}-{risk_group}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Drop missing values\n",
    "            series = df_train[target_col].dropna()\n",
    "            if len(series) < 2:\n",
    "                print(f\"Insufficient data for {freq}-{risk_group}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Fit GARCH(1,1) with constant mean\n",
    "            am = arch_model(series, mean='Constant', vol='GARCH', p=1, q=1, dist='normal', rescale=False)\n",
    "            res = am.fit(disp='off')\n",
    "            \n",
    "            # Store the fitted model and its key parameters.\n",
    "            # Note: We save the last observed variance in level space (i.e. not in log).\n",
    "            models[freq][risk_group] = {\n",
    "                'model': res,\n",
    "                'params': res.params,  # Contains omega, alpha[1], beta[1], mu\n",
    "                'last_var': res.conditional_volatility.iloc[-1]**2  \n",
    "            }\n",
    "            \n",
    "            # Save parameters for summary\n",
    "            pars = res.params\n",
    "            row = {\n",
    "                'Frequency': freq,\n",
    "                'Risk Group': risk_group,\n",
    "                'Omega': pars.get('omega', np.nan),\n",
    "                'Alpha[1]': pars.get('alpha[1]', np.nan),\n",
    "                'Beta[1]': pars.get('beta[1]', np.nan),\n",
    "                'Mean': pars.get('mu', np.nan)\n",
    "            }\n",
    "            model_summary = pd.concat([model_summary, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    return models, model_summary\n",
    "\n",
    "\n",
    "\n",
    "models, model_summary = train_models_by_frequency_and_risk(train_data_split, ['hourly', '3hourly', 'daily'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Frequency",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Risk Group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Omega",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Alpha[1]",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Beta[1]",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Mean",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a82506da-c5ee-47ec-aa11-985d620b4140",
       "rows": [
        [
         "0",
         "hourly",
         "low",
         "8.937349928276323e-10",
         "0.1",
         "0.8",
         "2.29133277405415e-05"
        ],
        [
         "1",
         "hourly",
         "medium",
         "8.627475252946855e-08",
         "0.2",
         "0.49999999999999994",
         "4.562336831372725e-05"
        ],
        [
         "2",
         "hourly",
         "high",
         "5.343838827743225e-09",
         "0.1000000013942149",
         "0.8800000001858953",
         "-0.033973353584888726"
        ],
        [
         "3",
         "3hourly",
         "low",
         "6.962347093854085e-10",
         "0.2",
         "0.78",
         "6.869935836559093e-05"
        ],
        [
         "4",
         "3hourly",
         "medium",
         "1.503358059668475e-07",
         "0.2",
         "0.7",
         "0.00013679715061403524"
        ],
        [
         "5",
         "3hourly",
         "high",
         "2.0980170384056952e-08",
         "0.2",
         "0.78",
         "0.00029641617373702854"
        ],
        [
         "6",
         "daily",
         "low",
         "1.0890356575295678e-08",
         "0.2",
         "0.78",
         "0.0005488568091107209"
        ],
        [
         "7",
         "daily",
         "medium",
         "3.0908914635680876e-07",
         "0.2",
         "0.78",
         "0.001093333233040394"
        ],
        [
         "8",
         "daily",
         "high",
         "3.6608645798994003e-07",
         "0.2",
         "0.78",
         "0.0023680761762030866"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Risk Group</th>\n",
       "      <th>Omega</th>\n",
       "      <th>Alpha[1]</th>\n",
       "      <th>Beta[1]</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hourly</td>\n",
       "      <td>low</td>\n",
       "      <td>8.937350e-10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hourly</td>\n",
       "      <td>medium</td>\n",
       "      <td>8.627475e-08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hourly</td>\n",
       "      <td>high</td>\n",
       "      <td>5.343839e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.033973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>low</td>\n",
       "      <td>6.962347e-10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.503358e-07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>high</td>\n",
       "      <td>2.098017e-08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.000296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>daily</td>\n",
       "      <td>low</td>\n",
       "      <td>1.089036e-08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daily</td>\n",
       "      <td>medium</td>\n",
       "      <td>3.090891e-07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.001093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>daily</td>\n",
       "      <td>high</td>\n",
       "      <td>3.660865e-07</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.002368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Frequency Risk Group         Omega  Alpha[1]  Beta[1]      Mean\n",
       "0    hourly        low  8.937350e-10       0.1     0.80  0.000023\n",
       "1    hourly     medium  8.627475e-08       0.2     0.50  0.000046\n",
       "2    hourly       high  5.343839e-09       0.1     0.88 -0.033973\n",
       "3   3hourly        low  6.962347e-10       0.2     0.78  0.000069\n",
       "4   3hourly     medium  1.503358e-07       0.2     0.70  0.000137\n",
       "5   3hourly       high  2.098017e-08       0.2     0.78  0.000296\n",
       "6     daily        low  1.089036e-08       0.2     0.78  0.000549\n",
       "7     daily     medium  3.090891e-07       0.2     0.78  0.001093\n",
       "8     daily       high  3.660865e-07       0.2     0.78  0.002368"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing rolling window\n",
    "Rolling window is used for a one step ahead forecast. So we constantly update the lagged data with an update lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_predictions(X_test, y_test, model, window_size=24, step_ahead=1):\n",
    "    params = model['params']\n",
    "    mu    = params.get('mu', 0.0)\n",
    "    omega = params.get('omega', np.nan)\n",
    "    alpha = params.get('alpha[1]', np.nan)\n",
    "    beta  = params.get('beta[1]', np.nan)\n",
    "    \n",
    "    # Start with the last observed variance from training (in level space)\n",
    "    sigma_sq_prev = model['last_var']\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    \n",
    "    # Loop over the test sample, starting from the window_size index.\n",
    "    max_index = len(X_test) - step_ahead\n",
    "    for i in range(window_size, max_index + 1):\n",
    "        sigma_sq = sigma_sq_prev\n",
    "        \n",
    "        # Forecast step-by-step in level space.\n",
    "        for h in range(step_ahead):\n",
    "            # Convert the test value from log-space to level.\n",
    "            r_t = np.exp(y_test.iloc[i + h - 1])\n",
    "            sigma_sq = omega + alpha * (r_t - mu)**2 + beta * sigma_sq\n",
    "        \n",
    "        # Forecasted variance (level)\n",
    "        forecast_value = sigma_sq\n",
    "        predictions.append(forecast_value)\n",
    "        \n",
    "        # Actual value: convert y_test from log-space to level\n",
    "        actual_val = np.exp(y_test.iloc[i + step_ahead - 1])\n",
    "        actuals.append(actual_val)\n",
    "        \n",
    "        dates.append(X_test['Date'].iloc[i + step_ahead - 1])\n",
    "        \n",
    "        # Update sigma_sq_prev using the current observation (converted to level)\n",
    "        sigma_sq_prev = np.exp(y_test.iloc[i])\n",
    "    \n",
    "    return np.array(predictions), np.array(actuals), np.array(dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing an evaluatation function\n",
    "This function evaluates the findings and puts it in a df for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_on_test_data(test_data_split, models, frequencies, window_size=24):\n",
    "    evaluation_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Ticker', 'MSE', 'R²'])\n",
    "    detailed_results = pd.DataFrame(columns=['Date', 'Ticker', 'Risk Group', 'Frequency', 'Predicted', 'Actual'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        # Define target column and forecast horizon based on frequency\n",
    "        if freq == 'hourly':\n",
    "            target = 'hourly_rv'\n",
    "            step_ahead = 1\n",
    "        elif freq == '3hourly':\n",
    "            target = '3_hourly_rv'\n",
    "            step_ahead = 3\n",
    "        elif freq == 'daily':\n",
    "            target = 'daily_rv'\n",
    "            step_ahead = 24\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported frequency: {freq}\")\n",
    "        \n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            if risk_group not in models[freq]:\n",
    "                continue\n",
    "            model_info = models[freq][risk_group]\n",
    "            group_data = test_data_split[risk_group].copy()\n",
    "            \n",
    "            unique_tickers = group_data['Ticker'].unique()\n",
    "            for ticker in unique_tickers:\n",
    "                ticker_data = group_data[group_data['Ticker'] == ticker].copy()\n",
    "                if target not in ticker_data.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Prepare test data with 'Date' and target column\n",
    "                X_test = ticker_data[['Date', target]].dropna()\n",
    "                y_test = X_test[target]\n",
    "                if len(X_test) < window_size + step_ahead:\n",
    "                    print(f\"Skipping {ticker}: insufficient data for {freq}-{risk_group}\")\n",
    "                    continue\n",
    "                \n",
    "                # Obtain forecasts using our rolling window prediction function\n",
    "                predictions, actuals, dates = rolling_window_predictions(X_test, y_test, model_info, window_size, step_ahead)\n",
    "                \n",
    "                # Align lengths if necessary\n",
    "                min_len = min(len(predictions), len(actuals))\n",
    "                predictions = predictions[:min_len]\n",
    "                actuals = actuals[:min_len]\n",
    "                dates = dates[:min_len]\n",
    "                \n",
    "                # Filter out any NaNs\n",
    "                valid_idx = ~np.isnan(predictions) & ~np.isnan(actuals)\n",
    "                predictions = predictions[valid_idx]\n",
    "                actuals = actuals[valid_idx]\n",
    "                dates = dates[valid_idx]\n",
    "                if len(predictions) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Convert both predictions and actuals to log-space for metric calculation.\n",
    "                log_predictions = np.log(predictions)\n",
    "                log_actuals = np.log(actuals)\n",
    "                \n",
    "                mse = mean_squared_error(log_actuals, log_predictions)\n",
    "                r2 = r2_score(log_actuals, log_predictions)\n",
    "                \n",
    "                eval_row = {\n",
    "                    'Frequency': freq,\n",
    "                    'Risk Group': risk_group.capitalize(),\n",
    "                    'Ticker': ticker,\n",
    "                    'MSE': mse,\n",
    "                    'R²': r2\n",
    "                }\n",
    "                evaluation_summary = pd.concat([evaluation_summary, pd.DataFrame([eval_row])], ignore_index=True)\n",
    "                \n",
    "                ticker_results = pd.DataFrame({\n",
    "                    'Date': dates,\n",
    "                    'Ticker': ticker,\n",
    "                    'Risk Group': risk_group.capitalize(),\n",
    "                    'Frequency': freq,\n",
    "                    'Predicted': predictions,  # in level (variance) space\n",
    "                    'Actual': actuals         # in level (variance) space\n",
    "                })\n",
    "                detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
    "    \n",
    "    return evaluation_summary, detailed_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define frequencies\n",
    "frequencies = ['hourly', '3hourly', 'daily']\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_summary, detailed_results = evaluate_models_on_test_data(\n",
    "    test_data_split, models, frequencies\n",
    ")\n",
    "\n",
    "print(detailed_results.head())\n",
    "# Save results\n",
    "detailed_results.to_csv('../../results/garch.csv', index=False)\n",
    "print(\"Evaluation Summary:\")\n",
    "print(evaluation_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
