{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR Model\n",
    "For each risk classficiation, we will train a model to fit to predict the RV model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries and data\n",
    "To obtain the data, please go to notebooks/data_preprocessing, and then run data_import.ipynb and then run data_preprocessing.ipynb. This will give you data/processed_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from arch import arch_model\n",
    "\n",
    "hourly_data = pd.read_csv('../..//data/hourly_data.csv')\n",
    "three_hourly_data = pd.read_csv('../../data/three_hourly_data.csv')\n",
    "daily_data = pd.read_csv('../../data/daily_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "Now we will use a different train-test split from the group project\n",
    "Group project: 80/20 split\n",
    "Individual: Use 1 year of training data, then use rolling window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train-test split\n",
    "# Sort the data by date\n",
    "hourly_data['Date'] = pd.to_datetime(hourly_data['Date'])\n",
    "three_hourly_data['Date'] = pd.to_datetime(three_hourly_data['Date'])\n",
    "daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "hourly_data = hourly_data.sort_values('Date')\n",
    "\n",
    "# Determine when the first year ends, and use it as train data\n",
    "# The rest of the data is used as test data\n",
    "min_date = hourly_data['Date'].min()\n",
    "max_date = hourly_data['Date'].max()\n",
    "\n",
    "# Calculate the total time span of the data\n",
    "total_time_span = max_date - min_date\n",
    "\n",
    "# Define the first year of data\n",
    "first_year_end = min_date + pd.DateOffset(years=1)\n",
    "\n",
    "# Filter data for the first year\n",
    "first_year_data = hourly_data[hourly_data['Date'] <= first_year_end]\n",
    "\n",
    "# Calculate the percentage of data in the first year\n",
    "percentage_first_year = (len(first_year_data) / len(hourly_data))\n",
    "\n",
    "train_split = percentage_first_year\n",
    "\n",
    "# Hourly data train-test split\n",
    "X_train_hourly = hourly_data[hourly_data['Date'] <= first_year_end]\n",
    "X_test_hourly = hourly_data[hourly_data['Date'] > first_year_end]\n",
    "\n",
    "# Three hourly data train-test split\n",
    "X_train_three_hourly = three_hourly_data[three_hourly_data['Date'] <= first_year_end]\n",
    "X_test_three_hourly = three_hourly_data[three_hourly_data['Date'] > first_year_end]\n",
    "\n",
    "# Daily data train-test split\n",
    "X_train_daily = daily_data[daily_data['Date'] <= first_year_end]\n",
    "X_test_daily = daily_data[daily_data['Date'] > first_year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date_split_hourly = {\n",
    "    'low': X_train_hourly[X_train_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_hourly[X_train_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_hourly[X_train_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_hourly = {\n",
    "    'low': X_test_hourly[X_test_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_hourly[X_test_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_hourly[X_test_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "train_date_split_three_hourly = {\n",
    "    'low': X_train_three_hourly[X_train_three_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_three_hourly[X_train_three_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_three_hourly[X_train_three_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_three_hourly = {\n",
    "    'low': X_test_three_hourly[X_test_three_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_three_hourly[X_test_three_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_three_hourly[X_test_three_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "train_date_split_daily = {\n",
    "    'low': X_train_daily[X_train_daily['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_daily[X_train_daily['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_daily[X_train_daily['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_daily = {\n",
    "    'low': X_test_daily[X_test_daily['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_daily[X_test_daily['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_daily[X_test_daily['Risk'] == 'High Risk']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model based on their classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wil give us 3 models to work with: model_low, model_medium, and model_high. We will use these subsequent models on the test data to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\85775861.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, pd.DataFrame([row])], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\85775861.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, pd.DataFrame([row])], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\85775861.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def train_models_by_frequency_and_risk(train_data, frequencies):\n",
    "    models = {}\n",
    "    model_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Omega', 'Alpha[1]', 'Beta[1]', 'Mean'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        models[freq] = {}\n",
    "        if freq == 'hourly':\n",
    "            target_col = 'ln_hourly_rv'\n",
    "        elif freq == '3hourly':\n",
    "            target_col = 'ln_3_hourly_rv'\n",
    "        elif freq == 'daily':\n",
    "            target_col = 'ln_daily_rv'\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported frequency: {freq}\")\n",
    "            \n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            df_train = train_data[risk_group].copy()\n",
    "            if target_col not in df_train.columns:\n",
    "                print(f\"Missing {target_col} for {freq}-{risk_group}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            series = df_train[target_col].dropna()\n",
    "            if len(series) < 2:\n",
    "                print(f\"Insufficient data for {freq}-{risk_group}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Fit GARCH(1,1) with constant mean on the ln-transformed series\n",
    "            am = arch_model(series, mean='Constant', vol='GARCH', p=1, q=1, dist='normal', rescale=False)\n",
    "            res = am.fit(disp='off')\n",
    "            \n",
    "            models[freq][risk_group] = {\n",
    "                'model': res,\n",
    "                'params': res.params,  # Contains omega, alpha[1], beta[1], mu\n",
    "                'last_var': res.conditional_volatility.iloc[-1]**2  \n",
    "            }\n",
    "            \n",
    "            pars = res.params\n",
    "            row = {\n",
    "                'Frequency': freq,\n",
    "                'Risk Group': risk_group,\n",
    "                'Omega': pars.get('omega', np.nan),\n",
    "                'Alpha[1]': pars.get('alpha[1]', np.nan),\n",
    "                'Beta[1]': pars.get('beta[1]', np.nan),\n",
    "                'Mean': pars.get('mu', np.nan)\n",
    "            }\n",
    "            model_summary = pd.concat([model_summary, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    return models, model_summary\n",
    "\n",
    "model_daily, summary_daily = train_models_by_frequency_and_risk(train_date_split_daily, ['daily'])\n",
    "model_hourly, summary_hourly = train_models_by_frequency_and_risk(train_date_split_hourly, ['hourly'])\n",
    "model_three_hourly, summary_three_hourly = train_models_by_frequency_and_risk(train_date_split_three_hourly, ['3hourly'])\n",
    "\n",
    "# Merge the summaries\n",
    "summary = pd.concat([summary_daily, summary_hourly, summary_three_hourly], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Frequency Risk Group     Omega  Alpha[1]   Beta[1]       Mean\n",
      "0     daily        low  0.037582  0.057627  0.914335  -7.997750\n",
      "1     daily     medium  0.246348  0.071943  0.732727  -7.484376\n",
      "2     daily       high  0.200376  0.221807  0.650649  -6.879797\n",
      "3    hourly        low  4.892120  0.163660  0.101320 -12.915812\n",
      "4    hourly     medium  0.298307  0.013675  0.940431 -12.266809\n",
      "5    hourly       high  0.023704  0.003597  0.993066 -11.660366\n",
      "6   3hourly        low  0.458925  0.124402  0.700717 -10.775833\n",
      "7   3hourly     medium  0.699855  0.127292  0.575893 -10.184531\n",
      "8   3hourly       high  0.078554  0.060181  0.911660  -9.567104\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing rolling window\n",
    "Rolling window is used for a one step ahead forecast. So we constantly update the lagged data with an update lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_predictions(X_test, y_test, model, window_size=24, step_ahead=1):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    \n",
    "    max_index = len(X_test) - step_ahead  # Ensure we have data for the forecast horizon\n",
    "    for i in range(window_size, max_index + 1):\n",
    "        # Use the target series within the current rolling window\n",
    "        window_data = y_test.iloc[i - window_size : i]\n",
    "        \n",
    "        # Re-fit the GARCH(1,1) model on the current window.\n",
    "        # We use the same specification as in training.\n",
    "        am = arch_model(window_data, mean='Constant', vol='GARCH', p=1, q=1, dist='normal', rescale=False)\n",
    "        res = am.fit(disp='off')\n",
    "        \n",
    "        # Forecast one step ahead using the re-fitted model.\n",
    "        forecast = res.forecast(horizon=step_ahead, reindex=False)\n",
    "        forecast_value = forecast.mean.iloc[-1, 0]\n",
    "        predictions.append(forecast_value)\n",
    "        \n",
    "        # Get the actual value at the forecast horizon.\n",
    "        actual_index = i + step_ahead - 1\n",
    "        actual_value = y_test.iloc[actual_index]\n",
    "        actuals.append(actual_value)\n",
    "        \n",
    "        # Capture the corresponding date from the test data.\n",
    "        forecast_date = X_test['Date'].iloc[actual_index]\n",
    "        dates.append(forecast_date)\n",
    "    \n",
    "    return np.array(predictions), np.array(actuals), np.array(dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing an evaluatation function\n",
    "This function evaluates the findings and puts it in a df for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_on_test_data(test_data_split, models, frequencies, window_size=24, step_ahead=1):\n",
    "    evaluation_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Ticker', 'MSE', 'R²'])\n",
    "    detailed_results = pd.DataFrame(columns=['Date', 'Ticker', 'Risk Group', 'Frequency', 'Predicted', 'Actual'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        # Define target and forecast horizon based on frequency.\n",
    "        if freq == 'hourly':\n",
    "            target = 'ln_hourly_rv'\n",
    "        elif freq == '3hourly':\n",
    "            target = 'ln_3_hourly_rv'\n",
    "        elif freq == 'daily':\n",
    "            target = 'ln_daily_rv'\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported frequency: {freq}\")\n",
    "        \n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            if risk_group not in models[freq]:\n",
    "                continue\n",
    "            model_info = models[freq][risk_group]\n",
    "            group_data = test_data_split[risk_group].copy()\n",
    "            \n",
    "            unique_tickers = group_data['Ticker'].unique()\n",
    "            for ticker in unique_tickers:\n",
    "                ticker_data = group_data[group_data['Ticker'] == ticker].copy()\n",
    "                if target not in ticker_data.columns:\n",
    "                    print(f\"Skipping {ticker}: missing target {target} for {freq} - {risk_group}\")\n",
    "                    continue\n",
    "                \n",
    "                X_test = ticker_data[['Date', target]].dropna()\n",
    "                y_test = X_test[target]\n",
    "                if len(X_test) < window_size + step_ahead:\n",
    "                    print(f\"Skipping {ticker}: insufficient data for {freq}-{risk_group} ({len(X_test)} rows)\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate forecasts using the GARCH forecast-based rolling window prediction function.\n",
    "                predictions, actuals, dates = rolling_window_predictions(\n",
    "                    X_test, y_test, model_info, window_size=window_size, step_ahead=step_ahead\n",
    "                )\n",
    "                \n",
    "                valid_idx = ~np.isnan(predictions) & ~np.isnan(actuals)\n",
    "                predictions = predictions[valid_idx]\n",
    "                actuals = actuals[valid_idx]\n",
    "                dates = dates[valid_idx]\n",
    "                if len(predictions) == 0:\n",
    "                    continue\n",
    "                \n",
    "                mse = mean_squared_error(actuals, predictions)\n",
    "                r2 = r2_score(actuals, predictions)\n",
    "                \n",
    "                eval_row = pd.DataFrame({\n",
    "                    'Frequency': [freq],\n",
    "                    'Risk Group': [risk_group.capitalize()],\n",
    "                    'Ticker': [ticker],\n",
    "                    'MSE': [mse],\n",
    "                    'R²': [r2]\n",
    "                })\n",
    "                evaluation_summary = pd.concat([evaluation_summary, eval_row], ignore_index=True)\n",
    "                \n",
    "                ticker_results = pd.DataFrame({\n",
    "                    'Date': dates,\n",
    "                    'Ticker': ticker,\n",
    "                    'Risk Group': risk_group.capitalize(),\n",
    "                    'Frequency': freq,\n",
    "                    'Predicted': predictions,\n",
    "                    'Actual': actuals\n",
    "                })\n",
    "                detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
    "    \n",
    "    return evaluation_summary, detailed_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\22485321.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat([evaluation_summary, eval_row], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\22485321.py:72: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\22485321.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat([evaluation_summary, eval_row], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\22485321.py:72: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\22485321.py:62: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat([evaluation_summary, eval_row], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_28088\\22485321.py:72: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Evaluation Summary:\n",
      "                           Date   Ticker Risk Group Frequency  Predicted  \\\n",
      "0     2024-04-02 01:00:00+00:00  BTC-USD        Low    hourly -12.434889   \n",
      "1     2024-04-02 02:00:00+00:00  BTC-USD        Low    hourly -12.596374   \n",
      "2     2024-04-02 03:00:00+00:00  BTC-USD        Low    hourly -12.247775   \n",
      "3     2024-04-02 04:00:00+00:00  BTC-USD        Low    hourly -12.224047   \n",
      "4     2024-04-02 05:00:00+00:00  BTC-USD        Low    hourly -12.169358   \n",
      "...                         ...      ...        ...       ...        ...   \n",
      "56210 2025-03-05 00:00:00+00:00  SOL-USD       High     daily  -6.084079   \n",
      "56211 2025-03-06 00:00:00+00:00  SOL-USD       High     daily  -6.074680   \n",
      "56212 2025-03-07 00:00:00+00:00  SOL-USD       High     daily  -6.069487   \n",
      "56213 2025-03-08 00:00:00+00:00  SOL-USD       High     daily  -5.977923   \n",
      "56214 2025-03-09 00:00:00+00:00  SOL-USD       High     daily  -6.006356   \n",
      "\n",
      "          Actual  \n",
      "0     -14.731732  \n",
      "1      -6.711456  \n",
      "2     -10.996162  \n",
      "3     -12.080866  \n",
      "4     -12.165719  \n",
      "...          ...  \n",
      "56210  -6.159678  \n",
      "56211  -5.681569  \n",
      "56212  -4.881344  \n",
      "56213  -7.232146  \n",
      "56214  -6.212519  \n",
      "\n",
      "[56215 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "evaluation_summary_hourly, detailed_results_hourly = evaluate_models_on_test_data(\n",
    "    test_date_split_hourly, model_hourly, ['hourly'], window_size=24\n",
    ")\n",
    "evaluation_summary_three_hourly, detailed_results_three_hourly = evaluate_models_on_test_data(\n",
    "    test_date_split_three_hourly, model_three_hourly, ['3hourly'], window_size=24\n",
    ")\n",
    "evaluation_summary_daily, detailed_results_daily = evaluate_models_on_test_data(\n",
    "    test_date_split_daily, model_daily, ['daily'], window_size=24\n",
    ")\n",
    "\n",
    "# Combine the evaluation summaries across frequencies, if desired.\n",
    "combined_summary = pd.concat([evaluation_summary_hourly, evaluation_summary_three_hourly, evaluation_summary_daily], ignore_index=True)\n",
    "combined_details = pd.concat([detailed_results_hourly, detailed_results_three_hourly, detailed_results_daily], ignore_index=True)\n",
    "\n",
    "# Save the detailed results to CSV\n",
    "combined_details.to_csv('../../results/garch.csv', index=False)\n",
    "\n",
    "# Print the evaluation summaries\n",
    "print(\"Combined Evaluation Summary:\")\n",
    "print(combined_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
