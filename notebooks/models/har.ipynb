{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR Model\n",
    "For each risk classficiation, we will train a model to fit to predict the RV model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries and data\n",
    "To obtain the data, please go to notebooks/data_preprocessing, and then run data_import.ipynb and then run data_preprocessing.ipynb. This will give you data/processed_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker',\n",
      "       'ln_hourly_return', 'ln_3_hourly_return', 'ln_hourly_rv',\n",
      "       'ln_3_hourly_rv', 'ln_daily_rv', 'ln_weekly_rv', 'ln_monthly_rv',\n",
      "       'ln_daily_rv_lag1', 'ln_daily_rv_lag2', 'ln_weekly_rv_lag1',\n",
      "       'ln_weekly_rv_lag2', 'ln_monthly_rv_lag1', 'ln_monthly_rv_lag2',\n",
      "       'ln_hourly_rv_lag1', 'ln_3_hourly_rv_lag1', 'ln_hourly_rv_lag2',\n",
      "       'ln_3_hourly_rv_lag2', 'ln_hourly_return_lag1',\n",
      "       'ln_3_hourly_return_lag1', 'ln_hourly_return_lag2',\n",
      "       'ln_3_hourly_return_lag2', 'Risk'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "data = pd.read_csv('../../data/processed_data.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Remove NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Print columns\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "Now we will use a different train-test split from the group project\n",
    "Group project: 80/20 split\n",
    "Individual: Use 1 year of training data, then use rolling window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-03 00:00:00+00:00 2024-05-03 00:00:00+00:00\n",
      "2024-05-03 01:00:00+00:00 2025-03-10 23:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "# Train data: 0.8 of the data\n",
    "# Test data: 0.2 of the data\n",
    "# Sort the data by date\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Determine when the first year ends, and use it as train data\n",
    "# The rest of the data is used as test data\n",
    "min_date = data['Date'].min()\n",
    "max_date = data['Date'].max()\n",
    "\n",
    "# Calculate the total time span of the data\n",
    "total_time_span = max_date - min_date\n",
    "\n",
    "# Define the first year of data\n",
    "first_year_end = min_date + pd.DateOffset(years=1)\n",
    "\n",
    "# Filter data for the first year\n",
    "first_year_data = data[data['Date'] <= first_year_end]\n",
    "\n",
    "# Calculate the percentage of data in the first year\n",
    "percentage_first_year = (len(first_year_data) / len(data))\n",
    "\n",
    "train_split = percentage_first_year\n",
    "train_data = data[:int(train_split * len(data))]\n",
    "test_data = data[int(train_split * len(data)):]\n",
    "\n",
    "# Print train and test data date\n",
    "print(train_data['Date'].min(), train_data['Date'].max())\n",
    "print(test_data['Date'].min(), test_data['Date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further split the data based on the risk level\n",
    "There are low, medium, and high risk models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split train and test data by risk groups\n",
    "train_data_split = {\n",
    "    'low': train_data[train_data['Risk'] == 'Low Risk'],\n",
    "    'medium': train_data[train_data['Risk'] == 'Medium Risk'],\n",
    "    'high': train_data[train_data['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_data_split = {\n",
    "    'low': test_data[test_data['Risk'] == 'Low Risk'],\n",
    "    'medium': test_data[test_data['Risk'] == 'Medium Risk'],\n",
    "    'high': test_data[test_data['Risk'] == 'High Risk']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model based on their classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wil give us 3 models to work with: model_low, model_medium, and model_high. We will use these subsequent models on the test data to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_27532\\3592300760.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_27532\\3592300760.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_27532\\3592300760.py:43: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to train models and summarize them\n",
    "def train_models_by_frequency_and_risk(train_data, frequencies):\n",
    "    models = {}\n",
    "    model_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Intercept'] + \n",
    "                                 ['ln_hourly_rv_lag1', 'ln_3_hourly_rv_lag1', 'ln_daily_rv_lag1', \n",
    "                                  'ln_weekly_rv_lag1', 'ln_monthly_rv_lag1'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        models[freq] = {}\n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            group_data = train_data[risk_group]\n",
    "            \n",
    "            # Select features and target based on frequency\n",
    "            if freq == 'hourly':\n",
    "                features = ['ln_hourly_rv_lag1', 'ln_3_hourly_rv_lag1', 'ln_daily_rv_lag1']\n",
    "                target = 'ln_hourly_rv'\n",
    "            elif freq == '3hourly':\n",
    "                features = ['ln_3_hourly_rv_lag1', 'ln_daily_rv_lag1', 'ln_weekly_rv_lag1']\n",
    "                target = 'ln_3_hourly_rv'\n",
    "            elif freq == 'daily':\n",
    "                features = ['ln_daily_rv_lag1', 'ln_weekly_rv_lag1', 'ln_monthly_rv_lag1']\n",
    "                target = 'ln_daily_rv'\n",
    "            \n",
    "            X_train = group_data[features]\n",
    "            y_train = group_data[target]\n",
    "            \n",
    "            # Train the model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            models[freq][risk_group] = model\n",
    "            \n",
    "            # Extract coefficients and intercept\n",
    "            coef_dict = {\n",
    "                'Frequency': freq,\n",
    "                'Risk Group': risk_group,\n",
    "                'Intercept': model.intercept_\n",
    "            }\n",
    "            for feature in features:\n",
    "                coef_dict[feature] = model.coef_[features.index(feature)]\n",
    "            \n",
    "            # Convert the dictionary to a DataFrame and append to the summary\n",
    "            coef_df = pd.DataFrame([coef_dict])\n",
    "            model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n",
    "    \n",
    "    return models, model_summary\n",
    "\n",
    "\n",
    "# Define frequencies\n",
    "frequencies = ['hourly', '3hourly', 'daily']\n",
    "\n",
    "# Train models for each frequency and risk group\n",
    "models, model_summary = train_models_by_frequency_and_risk(train_data_split, frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Frequency",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Risk Group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Intercept",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ln_hourly_rv_lag1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ln_3_hourly_rv_lag1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ln_daily_rv_lag1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ln_weekly_rv_lag1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ln_monthly_rv_lag1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3f26d873-e7ff-4613-be2e-6632fea76382",
       "rows": [
        [
         "0",
         "hourly",
         "low",
         "-5.607595214237481",
         "0.1087014495578792",
         "0.31733196787845985",
         "0.3019717339134313",
         null,
         null
        ],
        [
         "1",
         "hourly",
         "medium",
         "-5.851580506585797",
         "0.09611890663273343",
         "0.2553642790448325",
         "0.3611244981705368",
         null,
         null
        ],
        [
         "2",
         "hourly",
         "high",
         "-4.6689287088287665",
         "0.07961368014966018",
         "0.24531904126228787",
         "0.5337686139283891",
         null,
         null
        ],
        [
         "3",
         "3hourly",
         "low",
         "-2.633203231534246",
         null,
         "0.36414677494305925",
         "0.18641419509810048",
         "0.3562924255116788",
         null
        ],
        [
         "4",
         "3hourly",
         "medium",
         "-3.397418954154401",
         null,
         "0.2837947832819445",
         "0.23348923025527618",
         "0.30723899275296035",
         null
        ],
        [
         "5",
         "3hourly",
         "high",
         "-2.4814523769951915",
         null,
         "0.25983480558052285",
         "0.32507763822449737",
         "0.35361440069112804",
         null
        ],
        [
         "6",
         "daily",
         "low",
         "-0.8574419890226626",
         null,
         null,
         "0.2930436848105908",
         "0.23421303481165462",
         "0.40249702022734063"
        ],
        [
         "7",
         "daily",
         "medium",
         "-2.039617750481984",
         null,
         null,
         "0.3266181684355904",
         "0.3077190145609876",
         "0.12086643978474884"
        ],
        [
         "8",
         "daily",
         "high",
         "-0.6568584394898167",
         null,
         null,
         "0.436706193872328",
         "0.2783152749170041",
         "0.21210351733168709"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 9
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Risk Group</th>\n",
       "      <th>Intercept</th>\n",
       "      <th>ln_hourly_rv_lag1</th>\n",
       "      <th>ln_3_hourly_rv_lag1</th>\n",
       "      <th>ln_daily_rv_lag1</th>\n",
       "      <th>ln_weekly_rv_lag1</th>\n",
       "      <th>ln_monthly_rv_lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hourly</td>\n",
       "      <td>low</td>\n",
       "      <td>-5.607595</td>\n",
       "      <td>0.108701</td>\n",
       "      <td>0.317332</td>\n",
       "      <td>0.301972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hourly</td>\n",
       "      <td>medium</td>\n",
       "      <td>-5.851581</td>\n",
       "      <td>0.096119</td>\n",
       "      <td>0.255364</td>\n",
       "      <td>0.361124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hourly</td>\n",
       "      <td>high</td>\n",
       "      <td>-4.668929</td>\n",
       "      <td>0.079614</td>\n",
       "      <td>0.245319</td>\n",
       "      <td>0.533769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>low</td>\n",
       "      <td>-2.633203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.364147</td>\n",
       "      <td>0.186414</td>\n",
       "      <td>0.356292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>medium</td>\n",
       "      <td>-3.397419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.283795</td>\n",
       "      <td>0.233489</td>\n",
       "      <td>0.307239</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>high</td>\n",
       "      <td>-2.481452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.259835</td>\n",
       "      <td>0.325078</td>\n",
       "      <td>0.353614</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>daily</td>\n",
       "      <td>low</td>\n",
       "      <td>-0.857442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.293044</td>\n",
       "      <td>0.234213</td>\n",
       "      <td>0.402497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daily</td>\n",
       "      <td>medium</td>\n",
       "      <td>-2.039618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.326618</td>\n",
       "      <td>0.307719</td>\n",
       "      <td>0.120866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>daily</td>\n",
       "      <td>high</td>\n",
       "      <td>-0.656858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.278315</td>\n",
       "      <td>0.212104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Frequency Risk Group  Intercept  ln_hourly_rv_lag1  ln_3_hourly_rv_lag1  \\\n",
       "0    hourly        low  -5.607595           0.108701             0.317332   \n",
       "1    hourly     medium  -5.851581           0.096119             0.255364   \n",
       "2    hourly       high  -4.668929           0.079614             0.245319   \n",
       "3   3hourly        low  -2.633203                NaN             0.364147   \n",
       "4   3hourly     medium  -3.397419                NaN             0.283795   \n",
       "5   3hourly       high  -2.481452                NaN             0.259835   \n",
       "6     daily        low  -0.857442                NaN                  NaN   \n",
       "7     daily     medium  -2.039618                NaN                  NaN   \n",
       "8     daily       high  -0.656858                NaN                  NaN   \n",
       "\n",
       "   ln_daily_rv_lag1  ln_weekly_rv_lag1  ln_monthly_rv_lag1  \n",
       "0          0.301972                NaN                 NaN  \n",
       "1          0.361124                NaN                 NaN  \n",
       "2          0.533769                NaN                 NaN  \n",
       "3          0.186414           0.356292                 NaN  \n",
       "4          0.233489           0.307239                 NaN  \n",
       "5          0.325078           0.353614                 NaN  \n",
       "6          0.293044           0.234213            0.402497  \n",
       "7          0.326618           0.307719            0.120866  \n",
       "8          0.436706           0.278315            0.212104  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing rolling window\n",
    "Rolling window is used for a one step ahead forecast. So we constantly update the lagged data with an update lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_predictions(X_test, y_test, model, window_size=24, step_ahead=1):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    \n",
    "    # Maximum index to avoid out-of-bounds\n",
    "    max_index = len(X_test) - step_ahead  # Ensure enough data for prediction\n",
    "    \n",
    "    for i in range(window_size, max_index + 1):\n",
    "        # Extract features from the rolling window (excluding 'Date')\n",
    "        X_window = X_test.drop(columns=['Date']).iloc[i - window_size:i]\n",
    "        \n",
    "        # Predict the next `step_ahead` step using the last row of the window\n",
    "        y_pred = model.predict(X_window.tail(1))[0]\n",
    "        \n",
    "        # Get the actual value `step_ahead` steps ahead of the current window\n",
    "        actual_index = i + step_ahead - 1  # Actual value's index\n",
    "        actual_value = y_test.iloc[actual_index]\n",
    "        \n",
    "        # Capture the date of the predicted value (actual's date)\n",
    "        current_date = X_test['Date'].iloc[actual_index]\n",
    "        \n",
    "        predictions.append(y_pred)\n",
    "        actuals.append(actual_value)\n",
    "        dates.append(current_date)\n",
    "    \n",
    "    return predictions, actuals, dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing an evaluatation function\n",
    "This function evaluates the findings and puts it in a df for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_on_test_data(test_data_split, models, frequencies, window_size=24):\n",
    "    evaluation_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Ticker', 'MSE', 'R²'])\n",
    "    detailed_results = pd.DataFrame(columns=['Date', 'Ticker', 'Risk Group', 'Frequency', 'Predicted', 'Actual'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            model = models[freq][risk_group]\n",
    "            group_data = test_data_split[risk_group]\n",
    "            \n",
    "            # Define features, target, and step_ahead based on frequency\n",
    "            if freq == 'hourly':\n",
    "                features = ['ln_hourly_rv_lag1', 'ln_3_hourly_rv_lag1', 'ln_daily_rv_lag1']\n",
    "                target = 'ln_hourly_rv'\n",
    "                step_ahead = 1  # Predict 1 hour ahead\n",
    "            elif freq == '3hourly':\n",
    "                features = ['ln_3_hourly_rv_lag1', 'ln_daily_rv_lag1', 'ln_weekly_rv_lag1']\n",
    "                target = 'ln_3_hourly_rv'\n",
    "                step_ahead = 3  # Predict 3 hours ahead\n",
    "            elif freq == 'daily':\n",
    "                features = ['ln_daily_rv_lag1', 'ln_weekly_rv_lag1', 'ln_monthly_rv_lag1']\n",
    "                target = 'ln_daily_rv'\n",
    "                step_ahead = 24  # Predict 24 hours (1 day) ahead\n",
    "            \n",
    "            unique_tickers = group_data['Ticker'].unique()\n",
    "            \n",
    "            for ticker in unique_tickers:\n",
    "                ticker_data = group_data[group_data['Ticker'] == ticker].copy()\n",
    "                \n",
    "                # Validate features and target\n",
    "                if not all(f in ticker_data.columns for f in features) or target not in ticker_data.columns:\n",
    "                    print(f\"Skipping {ticker}: missing features or target for {freq}-{risk_group}\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare test data (features + Date)\n",
    "                X_test = ticker_data[['Date'] + features].dropna()\n",
    "                y_test = ticker_data.loc[X_test.index, target]\n",
    "                \n",
    "                # Check data sufficiency\n",
    "                if len(X_test) < window_size + step_ahead:\n",
    "                    print(f\"Skipping {ticker}: insufficient data ({len(X_test)} rows)\")\n",
    "                    continue\n",
    "                \n",
    "                # Get predictions and metrics\n",
    "                predictions, actuals, dates = rolling_window_predictions(\n",
    "                    X_test, y_test, model, window_size=window_size, step_ahead=step_ahead\n",
    "                )\n",
    "                \n",
    "                if len(predictions) == 0:\n",
    "                    continue  # Skip if no valid predictions\n",
    "                \n",
    "                # Calculate metrics\n",
    "                mse = mean_squared_error(actuals, predictions)\n",
    "                r2 = r2_score(actuals, predictions)\n",
    "                \n",
    "                # Append to summary\n",
    "                evaluation_summary = pd.concat([\n",
    "                    evaluation_summary,\n",
    "                    pd.DataFrame({\n",
    "                        'Frequency': [freq],\n",
    "                        'Risk Group': [risk_group.capitalize()],\n",
    "                        'Ticker': [ticker],\n",
    "                        'MSE': [mse],\n",
    "                        'R²': [r2]\n",
    "                    })\n",
    "                ], ignore_index=True)\n",
    "                \n",
    "                # Append detailed results\n",
    "                ticker_results = pd.DataFrame({\n",
    "                    'Date': dates,\n",
    "                    'Ticker': ticker,\n",
    "                    'Risk Group': risk_group.capitalize(),\n",
    "                    'Frequency': freq,\n",
    "                    'Predicted': predictions,\n",
    "                    'Actual': actuals\n",
    "                })\n",
    "                detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
    "    \n",
    "    return evaluation_summary, detailed_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_27532\\3898392649.py:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat([\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_27532\\3898392649.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Summary:\n",
      "   Frequency Risk Group    Ticker       MSE        R²\n",
      "0     hourly        Low   BTC-USD  6.070857  0.051346\n",
      "1     hourly     Medium   ETH-USD  5.839966  0.043728\n",
      "2     hourly     Medium   XRP-USD  5.967356  0.163062\n",
      "3     hourly       High  DOGE-USD  5.946162  0.059781\n",
      "4     hourly       High   SOL-USD  5.885372  0.033263\n",
      "5    3hourly        Low   BTC-USD  2.137566  0.106937\n",
      "6    3hourly     Medium   ETH-USD  1.911466  0.118436\n",
      "7    3hourly     Medium   XRP-USD  1.927677  0.374738\n",
      "8    3hourly       High  DOGE-USD  1.825432  0.168951\n",
      "9    3hourly       High   SOL-USD  1.637846  0.090618\n",
      "10     daily        Low   BTC-USD  1.125800 -0.061194\n",
      "11     daily     Medium   ETH-USD  0.921843  0.025831\n",
      "12     daily     Medium   XRP-USD  1.253558  0.366974\n",
      "13     daily       High  DOGE-USD  0.895492  0.083126\n",
      "14     daily       High   SOL-USD  0.735294  0.022917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define frequencies\n",
    "frequencies = ['hourly', '3hourly', 'daily']\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_summary, detailed_results = evaluate_models_on_test_data(\n",
    "    test_data_split, models, frequencies, window_size=24\n",
    ")\n",
    "\n",
    "# Save results\n",
    "detailed_results.to_csv('../../data/har.csv', index=False)\n",
    "print(\"Evaluation Summary:\")\n",
    "print(evaluation_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
