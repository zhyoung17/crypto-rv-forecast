{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR Model\n",
    "For each risk classficiation, we will train a model to fit to predict the RV model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries and data\n",
    "To obtain the data, please go to notebooks/data_preprocessing, and then run data_import.ipynb and then run data_preprocessing.ipynb. This will give you data/processed_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
      "       'ln_3_hourly_return', '3_hourly_rv', 'ln_3_hourly_rv',\n",
      "       'ln_3_hourly_rv_lag1', 'ln_3_hourly_rv_lag2', 'ln_3_hourly_rv_lag3',\n",
      "       'ln_3_hourly_rv_lag4', 'ln_3_hourly_rv_lag8', 'Risk'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "hourly_data = pd.read_csv('../..//data/hourly_data.csv')\n",
    "three_hourly_data = pd.read_csv('../../data/three_hourly_data.csv')\n",
    "daily_data = pd.read_csv('../../data/daily_data.csv')\n",
    "\n",
    "print(three_hourly_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "Now we will use a different train-test split from the group project\n",
    "Group project: 80/20 split\n",
    "Individual: Use 1 year of training data, then use rolling window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train-test split\n",
    "# Sort the data by date\n",
    "hourly_data['Date'] = pd.to_datetime(hourly_data['Date'])\n",
    "three_hourly_data['Date'] = pd.to_datetime(three_hourly_data['Date'])\n",
    "daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "hourly_data = hourly_data.sort_values('Date')\n",
    "\n",
    "# Determine when the first year ends, and use it as train data\n",
    "# The rest of the data is used as test data\n",
    "min_date = hourly_data['Date'].min()\n",
    "max_date = hourly_data['Date'].max()\n",
    "\n",
    "# Calculate the total time span of the data\n",
    "total_time_span = max_date - min_date\n",
    "\n",
    "# Define the first year of data\n",
    "first_year_end = min_date + pd.DateOffset(years=1)\n",
    "\n",
    "# Filter data for the first year\n",
    "first_year_data = hourly_data[hourly_data['Date'] <= first_year_end]\n",
    "\n",
    "# Calculate the percentage of data in the first year\n",
    "percentage_first_year = (len(first_year_data) / len(hourly_data))\n",
    "\n",
    "train_split = percentage_first_year\n",
    "\n",
    "# Hourly data train-test split\n",
    "X_train_hourly = hourly_data[hourly_data['Date'] <= first_year_end]\n",
    "X_test_hourly = hourly_data[hourly_data['Date'] > first_year_end]\n",
    "\n",
    "# Three hourly data train-test split\n",
    "X_train_three_hourly = three_hourly_data[three_hourly_data['Date'] <= first_year_end]\n",
    "X_test_three_hourly = three_hourly_data[three_hourly_data['Date'] > first_year_end]\n",
    "\n",
    "# Daily data train-test split\n",
    "X_train_daily = daily_data[daily_data['Date'] <= first_year_end]\n",
    "X_test_daily = daily_data[daily_data['Date'] > first_year_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further split the data based on the risk level\n",
    "There are low, medium, and high risk models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date_split_hourly = {\n",
    "    'low': X_train_hourly[X_train_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_hourly[X_train_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_hourly[X_train_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_hourly = {\n",
    "    'low': X_test_hourly[X_test_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_hourly[X_test_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_hourly[X_test_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "train_date_split_three_hourly = {\n",
    "    'low': X_train_three_hourly[X_train_three_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_three_hourly[X_train_three_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_three_hourly[X_train_three_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_three_hourly = {\n",
    "    'low': X_test_three_hourly[X_test_three_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_three_hourly[X_test_three_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_three_hourly[X_test_three_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "train_date_split_daily = {\n",
    "    'low': X_train_daily[X_train_daily['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_daily[X_train_daily['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_daily[X_train_daily['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_daily = {\n",
    "    'low': X_test_daily[X_test_daily['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_daily[X_test_daily['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_daily[X_test_daily['Risk'] == 'High Risk']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model based on their classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wil give us 3 models to work with: model_low, model_medium, and model_high. We will use these subsequent models on the test data to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\2180503011.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\2180503011.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\2180503011.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def train_models_by_frequency_and_risk(train_data, frequencies):\n",
    "    models = {}\n",
    "    model_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Intercept'] +\n",
    "                                 ['ln_hourly_rv_lag1', 'ln_hourly_rv_lag8', 'ln_hourly_rv_lag24',\n",
    "                                  'ln_3_hourly_rv_lag1', 'ln_3_hourly_rv_lag4', 'ln_3_hourly_rv_lag8',\n",
    "                                  'ln_daily_rv_lag1', 'ln_weekly_rv_lag1', 'ln_monthly_rv_lag1'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        models[freq] = {}\n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            group_data = train_data[risk_group]\n",
    "            \n",
    "            # Select features and target based on frequency\n",
    "            if freq == 'hourly':\n",
    "                # Use hourly, 8-hourly, and 24-hourly lag features for hourly frequency\n",
    "                features = ['ln_hourly_rv_lag1', 'ln_hourly_rv_lag8', 'ln_hourly_rv_lag24']\n",
    "                target = 'ln_hourly_rv'\n",
    "            elif freq == '3hourly':\n",
    "                # Use 3 hourly, 12 hourly, and 24 hourly lag features for 3 hourly frequency\n",
    "                features = ['ln_3_hourly_rv_lag1', 'ln_3_hourly_rv_lag4', 'ln_3_hourly_rv_lag8']\n",
    "                target = 'ln_3_hourly_rv'\n",
    "            elif freq == 'daily':\n",
    "                # For daily, use the daily, weekly, and monthly lags.\n",
    "                features = ['ln_daily_rv_lag1', 'ln_weekly_rv_lag1', 'ln_monthly_rv_lag1']\n",
    "                target = 'ln_daily_rv'\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported frequency: {freq}\")\n",
    "            \n",
    "            # Ensure that group_data contains all required features\n",
    "            missing_features = [f for f in features if f not in group_data.columns]\n",
    "            if missing_features:\n",
    "                print(f\"Missing features {missing_features} for {freq}-{risk_group}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Create a mask for rows with no NaN in the selected features and target\n",
    "            mask = group_data[features + [target]].notnull().all(axis=1)\n",
    "            X_train = group_data.loc[mask, features]\n",
    "            y_train = group_data.loc[mask, target]\n",
    "            \n",
    "            # Check for sufficient data after dropping NaNs\n",
    "            if len(X_train) < 2:\n",
    "                print(f\"Insufficient data for {freq}-{risk_group} after dropping NaNs, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            models[freq][risk_group] = model\n",
    "            \n",
    "            # Build a summary of coefficients and intercept\n",
    "            coef_dict = {\n",
    "                'Frequency': freq,\n",
    "                'Risk Group': risk_group.capitalize(),\n",
    "                'Intercept': model.intercept_\n",
    "            }\n",
    "            for feature in features:\n",
    "                coef_dict[feature] = model.coef_[features.index(feature)]\n",
    "            \n",
    "            coef_df = pd.DataFrame([coef_dict])\n",
    "            model_summary = pd.concat([model_summary, coef_df], ignore_index=True)\n",
    "    \n",
    "    return models, model_summary\n",
    "\n",
    "# Define frequencies\n",
    "frequencies = ['hourly', '3hourly', 'daily']\n",
    "\n",
    "# Train models using your risk-split training dictionaries (ensure these dicts contain DataFrames, not boolean masks)\n",
    "model_hourly, summary_hourly = train_models_by_frequency_and_risk(train_date_split_hourly, ['hourly'])\n",
    "model_three_hourly, summary_three_hourly = train_models_by_frequency_and_risk(train_date_split_three_hourly, ['3hourly'])\n",
    "model_daily, summary_daily = train_models_by_frequency_and_risk(train_date_split_daily, ['daily'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Model Summary:\n",
      "  Frequency Risk Group  Intercept  ln_hourly_rv_lag1  ln_hourly_rv_lag8  \\\n",
      "0    hourly        Low  -8.158565           0.189469           0.092771   \n",
      "1    hourly     Medium  -8.651461           0.145536           0.073005   \n",
      "2    hourly       High  -6.556667           0.189176           0.116818   \n",
      "3   3hourly        Low  -3.803306                NaN                NaN   \n",
      "4   3hourly     Medium  -4.217920                NaN                NaN   \n",
      "5   3hourly       High  -2.498676                NaN                NaN   \n",
      "6     daily        Low  -1.217280                NaN                NaN   \n",
      "7     daily     Medium  -3.398506                NaN                NaN   \n",
      "8     daily       High  -0.708454                NaN                NaN   \n",
      "\n",
      "   ln_hourly_rv_lag24  ln_3_hourly_rv_lag1  ln_3_hourly_rv_lag4  \\\n",
      "0            0.084990                  NaN                  NaN   \n",
      "1            0.076400                  NaN                  NaN   \n",
      "2            0.131807                  NaN                  NaN   \n",
      "3                 NaN             0.343355             0.119652   \n",
      "4                 NaN             0.307883             0.141586   \n",
      "5                 NaN             0.328324             0.188328   \n",
      "6                 NaN                  NaN                  NaN   \n",
      "7                 NaN                  NaN                  NaN   \n",
      "8                 NaN                  NaN                  NaN   \n",
      "\n",
      "   ln_3_hourly_rv_lag8  ln_daily_rv_lag1  ln_weekly_rv_lag1  \\\n",
      "0                  NaN               NaN                NaN   \n",
      "1                  NaN               NaN                NaN   \n",
      "2                  NaN               NaN                NaN   \n",
      "3             0.185360               NaN                NaN   \n",
      "4             0.136030               NaN                NaN   \n",
      "5             0.222571               NaN                NaN   \n",
      "6                  NaN          0.274805           0.319564   \n",
      "7                  NaN          0.328018           0.293483   \n",
      "8                  NaN          0.424249           0.318190   \n",
      "\n",
      "   ln_monthly_rv_lag1  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n",
      "5                 NaN  \n",
      "6            0.286342  \n",
      "7           -0.067766  \n",
      "8            0.177717  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\2936980918.py:2: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_summary = pd.concat([summary_hourly, summary_three_hourly, summary_daily], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Merge summaries for an overall overview\n",
    "combined_summary = pd.concat([summary_hourly, summary_three_hourly, summary_daily], ignore_index=True)\n",
    "print(\"Combined Model Summary:\")\n",
    "print(combined_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing rolling window\n",
    "Rolling window is used for a one step ahead forecast. So we constantly update the lagged data with an update lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_predictions(X_test, y_test, model,features, window_size=24, step_ahead=1):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    \n",
    "    # Ensure we have enough rows to forecast.\n",
    "    max_index = len(X_test) - step_ahead + 1\n",
    "    for i in range(window_size, max_index):\n",
    "        # Use double brackets to return a DataFrame with the correct column names.\n",
    "        current_features = X_test.iloc[[i - 1]][features]\n",
    "        forecast_value = model.predict(current_features)[0]\n",
    "        predictions.append(forecast_value)\n",
    "        \n",
    "        # Actual value is taken at i + step_ahead - 1.\n",
    "        actual_index = i + step_ahead - 1\n",
    "        actuals.append(y_test.iloc[actual_index])\n",
    "        dates.append(X_test['Date'].iloc[actual_index])\n",
    "    \n",
    "    return predictions, actuals, dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing an evaluatation function\n",
    "This function evaluates the findings and puts it in a df for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_on_test_data(test_data_split, models, frequencies, window_size=24):\n",
    "    evaluation_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Ticker', 'MSE', 'R²'])\n",
    "    detailed_results = pd.DataFrame(columns=['Date', 'Ticker', 'Risk Group', 'Frequency', 'Predicted', 'Actual'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            # Check if a model exists for this frequency and risk group\n",
    "            if risk_group not in models[freq]:\n",
    "                continue\n",
    "            har_model = models[freq][risk_group]\n",
    "            group_data = test_data_split[risk_group].copy()\n",
    "            \n",
    "            # Define features, target, and forecast horizon based on frequency\n",
    "            if freq == 'hourly':\n",
    "                # Use hourly, 8-hourly, and 24-hourly lag features for hourly frequency\n",
    "                features = ['ln_hourly_rv_lag1', 'ln_hourly_rv_lag8', 'ln_hourly_rv_lag24']\n",
    "                target = 'ln_hourly_rv'\n",
    "            elif freq == '3hourly':\n",
    "                # Use 3 hourly, 12 hourly, and 24 hourly lag features for 3 hourly frequency\n",
    "                features = ['ln_3_hourly_rv_lag1', 'ln_3_hourly_rv_lag4', 'ln_3_hourly_rv_lag8']\n",
    "                target = 'ln_3_hourly_rv'\n",
    "            elif freq == 'daily':\n",
    "                # For daily, use the daily, weekly, and monthly lags.\n",
    "                features = ['ln_daily_rv_lag1', 'ln_weekly_rv_lag1', 'ln_monthly_rv_lag1']\n",
    "                target = 'ln_daily_rv'\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported frequency: {freq}\")\n",
    "            \n",
    "            unique_tickers = group_data['Ticker'].unique()\n",
    "            for ticker in unique_tickers:\n",
    "                ticker_data = group_data[group_data['Ticker'] == ticker].copy()\n",
    "                \n",
    "                # Validate that required columns exist\n",
    "                if not all(f in ticker_data.columns for f in features) or target not in ticker_data.columns:\n",
    "                    print(f\"Skipping {ticker}: missing features or target for {freq} - {risk_group}\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare test data using the required features and Date.\n",
    "                X_test = ticker_data[['Date'] + features].dropna()\n",
    "                y_test = ticker_data.loc[X_test.index, target]\n",
    "                \n",
    "                if len(X_test) < window_size + 1:\n",
    "                    print(f\"Skipping {ticker}: insufficient data ({len(X_test)} rows)\")\n",
    "                    continue\n",
    "                \n",
    "                # Generate forecasts using the rolling-window prediction function.\n",
    "                predictions, actuals, dates = rolling_window_predictions(\n",
    "                    X_test, y_test, har_model, features=features,\n",
    "                    window_size=window_size\n",
    "                )\n",
    "                if len(predictions) == 0:\n",
    "                    continue\n",
    "                                \n",
    "                # Calculate evaluation metrics.\n",
    "                mse = mean_squared_error(actuals, predictions)\n",
    "                r2 = r2_score(actuals, predictions)\n",
    "                \n",
    "                summary_row = pd.DataFrame({\n",
    "                    'Frequency': [freq],\n",
    "                    'Risk Group': [risk_group.capitalize()],\n",
    "                    'Ticker': [ticker],\n",
    "                    'MSE': [mse],\n",
    "                    'R²': [r2]\n",
    "                })\n",
    "                evaluation_summary = pd.concat([evaluation_summary, summary_row], ignore_index=True)\n",
    "                \n",
    "                ticker_results = pd.DataFrame({\n",
    "                    'Date': dates,\n",
    "                    'Ticker': ticker,\n",
    "                    'Risk Group': risk_group.capitalize(),\n",
    "                    'Frequency': freq,\n",
    "                    'Predicted': predictions,\n",
    "                    'Actual': actuals\n",
    "                })\n",
    "                detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
    "    \n",
    "    return evaluation_summary, detailed_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\643936983.py:65: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat([evaluation_summary, summary_row], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\643936983.py:75: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\643936983.py:65: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat([evaluation_summary, summary_row], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\643936983.py:75: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\643936983.py:65: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat([evaluation_summary, summary_row], ignore_index=True)\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_29248\\643936983.py:75: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Summary:\n",
      "   Frequency Risk Group    Ticker       MSE        R²\n",
      "0     hourly        Low   BTC-USD  6.285184  0.008847\n",
      "1     hourly        Low   ETH-USD  6.420979 -0.047947\n",
      "2     hourly     Medium   XRP-USD  6.625459  0.055388\n",
      "3     hourly       High  DOGE-USD  6.256185 -0.001267\n",
      "4     hourly       High   SOL-USD  6.124855 -0.013703\n",
      "5    3hourly        Low   BTC-USD  2.188624  0.074313\n",
      "6    3hourly        Low   ETH-USD  2.081993  0.033786\n",
      "7    3hourly     Medium   XRP-USD  2.079133  0.309614\n",
      "8    3hourly       High  DOGE-USD  1.920552  0.118030\n",
      "9    3hourly       High   SOL-USD  1.709443  0.067489\n",
      "10     daily        Low   BTC-USD  1.100746 -0.044790\n",
      "11     daily        Low   ETH-USD  0.897307  0.025271\n",
      "12     daily     Medium   XRP-USD  1.323332  0.318631\n",
      "13     daily       High  DOGE-USD  0.885850  0.081082\n",
      "14     daily       High   SOL-USD  0.726315  0.021742\n"
     ]
    }
   ],
   "source": [
    "# Define frequencies\n",
    "frequencies = ['hourly', '3hourly', 'daily']\n",
    "\n",
    "# Evaluate models on test data for each frequency.\n",
    "evaluation_summary_hourly, detailed_results_hourly = evaluate_models_on_test_data(\n",
    "    test_date_split_hourly, model_hourly, ['hourly'], window_size=24\n",
    ")\n",
    "evaluation_summary_three_hourly, detailed_results_three_hourly = evaluate_models_on_test_data(\n",
    "    test_date_split_three_hourly, model_three_hourly, ['3hourly'], window_size=24\n",
    ")\n",
    "evaluation_summary_daily, detailed_results_daily = evaluate_models_on_test_data(\n",
    "    test_date_split_daily, model_daily, ['daily'], window_size=24\n",
    ")\n",
    "\n",
    "# Combine the evaluation summaries across frequencies.\n",
    "combined_summary = pd.concat([evaluation_summary_hourly, evaluation_summary_three_hourly, evaluation_summary_daily], ignore_index=True)\n",
    "combined_details = pd.concat([detailed_results_hourly, detailed_results_three_hourly, detailed_results_daily], ignore_index=True)\n",
    "combined_details.to_csv('../../results/har.csv', index=False)\n",
    "print(\"Evaluation Summary:\")\n",
    "print(combined_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
