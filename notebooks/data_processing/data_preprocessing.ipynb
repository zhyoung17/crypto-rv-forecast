{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "data_route = \"../../data/raw_data.csv\"\n",
    "data = pd.read_csv(data_route)\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days of data for each ticker:\n",
      "Ticker\n",
      "BTC-USD     709\n",
      "DOGE-USD    709\n",
      "ETH-USD     709\n",
      "SOL-USD     709\n",
      "SPY         485\n",
      "XRP-USD     709\n",
      "Name: Date, dtype: int64\n",
      "\n",
      "Daily data with number of transactions:\n",
      "    Ticker                      Date          Open          High  \\\n",
      "0  BTC-USD 2023-04-01 00:00:00+00:00  28473.332031  28802.457031   \n",
      "1  BTC-USD 2023-04-02 00:00:00+00:00  28462.845703  28518.958984   \n",
      "2  BTC-USD 2023-04-03 00:00:00+00:00  28183.080078  28475.623047   \n",
      "3  BTC-USD 2023-04-04 00:00:00+00:00  27795.273438  28433.742188   \n",
      "4  BTC-USD 2023-04-05 00:00:00+00:00  28169.726562  28739.238281   \n",
      "5  BTC-USD 2023-04-06 00:00:00+00:00  28175.226562  28178.384766   \n",
      "6  BTC-USD 2023-04-07 00:00:00+00:00  28038.966797  28111.593750   \n",
      "7  BTC-USD 2023-04-08 00:00:00+00:00  27923.943359  28159.863281   \n",
      "8  BTC-USD 2023-04-09 00:00:00+00:00  27952.367188  28532.830078   \n",
      "9  BTC-USD 2023-04-10 00:00:00+00:00  28336.027344  29771.464844   \n",
      "\n",
      "            Low         Close        Volume  Daily_Transactions  \n",
      "0  28297.171875  28466.199219  3.055288e+08                  24  \n",
      "1  27884.087891  28202.138672  2.249324e+09                  24  \n",
      "2  27276.720703  27786.367188  7.748741e+09                  24  \n",
      "3  27681.304688  28170.050781  1.185264e+09                  24  \n",
      "4  27843.763672  28175.435547  3.515677e+09                  24  \n",
      "5  27765.341797  28034.193359  9.813422e+08                  24  \n",
      "6  27794.031250  27925.455078  2.678426e+08                  24  \n",
      "7  27883.386719  27945.478516  4.694487e+08                  24  \n",
      "8  27828.480469  28348.171875  3.246046e+09                  24  \n",
      "9  28189.271484  29650.720703  7.995150e+09                  24  \n",
      "\n",
      "Average daily number of transactions for each ticker:\n",
      "Ticker\n",
      "BTC-USD     23.913963\n",
      "DOGE-USD    23.904090\n",
      "ETH-USD     23.912553\n",
      "SOL-USD     23.912553\n",
      "SPY          6.958763\n",
      "XRP-USD     23.913963\n",
      "Name: Daily_Transactions, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "daily_data = (\n",
    "    data.groupby(['Ticker', pd.Grouper(key='Date', freq='D')])  # Group by Ticker and Date (daily frequency)\n",
    "    .agg({\n",
    "        'Open': 'first',   # First price of the day\n",
    "        'High': 'max',     # Highest price of the day\n",
    "        'Low': 'min',      # Lowest price of the day\n",
    "        'Close': 'last',   # Last price of the day\n",
    "        'Volume': 'sum',   # Total volume of the day\n",
    "        'Ticker': 'size'   # Count the number of hourly data points (transactions)\n",
    "    })\n",
    "    .rename(columns={'Ticker': 'Daily_Transactions'})  # Rename the count column\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the total number of days of data for each ticker\n",
    "days_per_ticker = daily_data.groupby('Ticker')['Date'].nunique()\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of days of data for each ticker:\")\n",
    "print(days_per_ticker)\n",
    "\n",
    "print(\"\\nDaily data with number of transactions:\")\n",
    "print(daily_data.head(10))\n",
    "\n",
    "# Average daily number of transactions\n",
    "avg_daily_transactions = daily_data.groupby('Ticker')['Daily_Transactions'].mean()\n",
    "print(\"\\nAverage daily number of transactions for each ticker:\")\n",
    "print(avg_daily_transactions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finding makes sense, as SPY does not operate over the weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering\n",
    "Now we have the calculate the key metrics, similar to how we did it in the group component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the hourly, 3-hourly, daily, weekly, and monthly \n",
    "Defined by Corsi 2009, weekly RV is the rolling average of 7 days RV, and monthly RV is the rolling average of 30 days RV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_18548\\4063719718.py:20: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  data.groupby(['Ticker', pd.Grouper(key='Date', freq='3H')])['ln_hourly_return']\n",
      "c:\\Users\\young\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure Date is datetime, which accounts for hourly data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Sort data by Ticker and Date\n",
    "data = data.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# Calculate hourly log returns\n",
    "data['ln_hourly_return'] = data.groupby('Ticker', group_keys=False)['Close'].apply(lambda x: np.log(x).diff())\n",
    "\n",
    "# Calculate 3-hourly log returns\n",
    "data['ln_3_hourly_return'] = data.groupby('Ticker', group_keys=False)['Close'].apply(lambda x: np.log(x).diff(3))\n",
    "\n",
    "# Fill the forward fill missing daily log returns\n",
    "# data['ln_daily_return'] = data.groupby('Ticker')['ln_daily_return'].ffill()\n",
    "# Calculate realized variances\n",
    "data['hourly_rv'] = data['ln_hourly_return']**2\n",
    "\n",
    "# 3-hourly RV: Sum of squared log returns over 3-hour windows\n",
    "data['3_hourly_rv'] = (\n",
    "    data.groupby(['Ticker', pd.Grouper(key='Date', freq='3H')])['ln_hourly_return']\n",
    "    .transform(lambda x: (x**2).sum())\n",
    ")\n",
    "\n",
    "# Daily RV: Sum of squared hourly returns per day\n",
    "data['Date_floor'] = data['Date'].dt.floor('D')  # Create a floored date column for daily grouping\n",
    "daily_rv = (\n",
    "    data.groupby(['Ticker', 'Date_floor'])['hourly_rv']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'hourly_rv': 'daily_rv'})\n",
    ")\n",
    "\n",
    "# Merge daily RV back into the original DataFrame\n",
    "data = data.merge(daily_rv, on=['Ticker', 'Date_floor'], how='left')\n",
    "\n",
    "# Logarithmic transformations for RV\n",
    "data['ln_hourly_rv'] = np.log(data['hourly_rv']).replace(-np.inf, 0)\n",
    "data['ln_3_hourly_rv'] = np.log(data['3_hourly_rv']).replace(-np.inf, 0)\n",
    "\n",
    "\n",
    "# Prepare daily DataFrame for weekly and monthly RV calculations\n",
    "daily_df = data[['Ticker', 'Date_floor', 'daily_rv']].drop_duplicates()\n",
    "\n",
    "# Calculate daily log RV\n",
    "daily_df['ln_daily_rv'] = np.log(daily_df['daily_rv']).replace(-np.inf, 0)\n",
    "\n",
    "# Calculate weekly RV (7-day rolling average)\n",
    "daily_df['weekly_rv'] = (\n",
    "    daily_df.groupby('Ticker')['daily_rv']\n",
    "    .transform(lambda x: x.rolling(window=7, min_periods=7).sum() / 7)\n",
    ")\n",
    "\n",
    "# Logarithmic transformation for weekly RV\n",
    "daily_df['ln_weekly_rv'] = np.log(daily_df['weekly_rv']).replace(-np.inf, 0)\n",
    "\n",
    "# Calculate monthly RV (30-day rolling average)\n",
    "daily_df['monthly_rv'] = (\n",
    "    daily_df.groupby('Ticker')['daily_rv']\n",
    "    .transform(lambda x: x.rolling(window=30, min_periods=30).sum() / 30)\n",
    ")\n",
    "# Logarithmic transformation for monthly RV\n",
    "daily_df['ln_monthly_rv'] = np.log(daily_df['monthly_rv']).replace(-np.inf, 0)\n",
    "\n",
    "# Lag the ln daily RV by one day\n",
    "daily_df['ln_daily_rv_lag1'] = daily_df.groupby('Ticker')['ln_daily_rv'].shift(1)\n",
    "daily_df['ln_daily_rv_lag2'] = daily_df.groupby('Ticker')['ln_daily_rv'].shift(2)\n",
    "\n",
    "# Lag the ln weekly, and monthly RV by one and two days\n",
    "daily_df['ln_weekly_rv_lag1'] = daily_df.groupby('Ticker')['ln_weekly_rv'].shift(1)\n",
    "daily_df['ln_weekly_rv_lag2'] = daily_df.groupby('Ticker')['ln_weekly_rv'].shift(2)\n",
    "daily_df['ln_monthly_rv_lag1'] = daily_df.groupby('Ticker')['ln_monthly_rv'].shift(1)\n",
    "daily_df['ln_monthly_rv_lag2'] = daily_df.groupby('Ticker')['ln_monthly_rv'].shift(2)\n",
    "\n",
    "# Merge weekly and monthly RV back into the original DataFrame\n",
    "data = data.merge(daily_df, on=['Ticker', 'Date_floor'], how='left')\n",
    "\n",
    "# Drop the temporary Date_floor column if no longer needed\n",
    "data.drop(columns=['Date_floor'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag the RV by 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag all realized variance measures by 1\n",
    "data['ln_hourly_rv_lag1'] = data.groupby('Ticker')['ln_hourly_rv'].shift(1)\n",
    "data['ln_3_hourly_rv_lag1'] = data.groupby('Ticker')['ln_3_hourly_rv'].shift(3)\n",
    "\n",
    "# Now lag all rv by 2\n",
    "data['ln_hourly_rv_lag2'] = data.groupby('Ticker')['ln_hourly_rv'].shift(2)\n",
    "data['ln_3_hourly_rv_lag2'] = data.groupby('Ticker')['ln_3_hourly_rv'].shift(6)\n",
    "\n",
    "# Lag the returns by 1 too\n",
    "data['ln_hourly_return_lag1'] = data.groupby('Ticker')['ln_hourly_return'].shift(1)\n",
    "data['ln_3_hourly_return_lag1'] = data.groupby('Ticker')['ln_3_hourly_return'].shift(3)\n",
    "\n",
    "# Lag the returns by 2 too\n",
    "data['ln_hourly_return_lag2'] = data.groupby('Ticker')['ln_hourly_return'].shift(2)\n",
    "data['ln_3_hourly_return_lag2'] = data.groupby('Ticker')['ln_3_hourly_return'].shift(6)\n",
    "\n",
    "data['hourly_rv_lag1'] = data.groupby('Ticker')['hourly_rv'].shift(1)\n",
    "data['hourly_rv_lag2'] = data.groupby('Ticker')['hourly_rv'].shift(2)\n",
    "\n",
    "data['three_hourly_rv_lag1'] = data.groupby('Ticker')['3_hourly_rv'].shift(3)\n",
    "data['three_hourly_rv_lag2'] = data.groupby('Ticker')['3_hourly_rv'].shift(6)\n",
    "\n",
    "# Save the data to a new CSV file\n",
    "data.to_csv('../../data/processed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop all non-lg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename daily_rv_x to daily_rv, and drop the other daily_rv_y column\n",
    "data['daily_rv'] = data['daily_rv_x']\n",
    "data = data.drop(columns=['daily_rv_x', 'daily_rv_y'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "In my model, I wish to classify each coin as either high risk, medium risk, or low risk, based on their hourly realised variance. High RV constitutes as high risk, and likewise for medium risk and low risk. I've used a weighting system to determine the risk level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Ticker  ln_hourly_rv  ln_3_hourly_rv  ln_daily_rv  Composite_Score  \\\n",
      "0   BTC-USD    -12.712268      -10.660145    -7.940902        -9.710948   \n",
      "2   ETH-USD    -12.349248      -10.259421    -7.547093        -9.321223   \n",
      "5   XRP-USD    -11.921673       -9.850787    -7.171542        -8.925341   \n",
      "1  DOGE-USD    -11.577992       -9.501986    -6.815516        -8.573952   \n",
      "3   SOL-USD    -11.167331       -9.110837    -6.502131        -8.217783   \n",
      "4       SPY    -13.597308      -11.926745   -10.257267       -11.426119   \n",
      "\n",
      "          Risk  \n",
      "0     Low Risk  \n",
      "2  Medium Risk  \n",
      "5  Medium Risk  \n",
      "1    High Risk  \n",
      "3    High Risk  \n",
      "4     Baseline  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Aggregate hourly, 3-hourly, and daily realized variance for each ticker\n",
    "ticker_rv_aggregated = (\n",
    "    data.groupby('Ticker')\n",
    "    .agg({\n",
    "        'ln_hourly_rv': 'mean',\n",
    "        'ln_3_hourly_rv': 'mean',\n",
    "        'ln_daily_rv': 'mean'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Define weights for each metric --> Give more weights to daily RV, since it's more stable\n",
    "weights = {\n",
    "    'ln_hourly_rv': 0.2,  # 20% weight\n",
    "    'ln_3_hourly_rv': 0.3,  # 30% weight\n",
    "    'ln_daily_rv': 0.5  # 50% weight\n",
    "}\n",
    "\n",
    "# Calculate the composite score\n",
    "ticker_rv_aggregated['Composite_Score'] = (\n",
    "    ticker_rv_aggregated['ln_hourly_rv'] * weights['ln_hourly_rv'] +\n",
    "    ticker_rv_aggregated['ln_3_hourly_rv'] * weights['ln_3_hourly_rv'] +\n",
    "    ticker_rv_aggregated['ln_daily_rv'] * weights['ln_daily_rv']\n",
    ")\n",
    "\n",
    "# Define dynamic bins based on the composite score\n",
    "min_score = ticker_rv_aggregated['Composite_Score'].min()\n",
    "max_score = ticker_rv_aggregated['Composite_Score'].max()\n",
    "\n",
    "# Create 3 bins (Low, Medium, High) using percentiles or custom logic\n",
    "bins = [\n",
    "    min_score, \n",
    "    ticker_rv_aggregated['Composite_Score'].quantile(0.33),  # 33rd percentile\n",
    "    ticker_rv_aggregated['Composite_Score'].quantile(0.66),  # 66th percentile\n",
    "    max_score\n",
    "]\n",
    "\n",
    "bins = sorted(list(set(bins)))  # Remove duplicates and sort\n",
    "\n",
    "# Classify tickers\n",
    "risk_labels = ['Low Risk', 'Medium Risk', 'High Risk']  # Initial categories\n",
    "ticker_rv_aggregated['Risk'] = pd.cut(\n",
    "    ticker_rv_aggregated['Composite_Score'],\n",
    "    bins=bins,\n",
    "    labels=risk_labels,  # Use the initial categories\n",
    "    include_lowest=True  # Include the minimum value in the first bin\n",
    ")\n",
    "\n",
    "# Add \"Baseline\" to the categories of the 'Risk' column\n",
    "ticker_rv_aggregated['Risk'] = ticker_rv_aggregated['Risk'].cat.add_categories('Baseline')\n",
    "\n",
    "# For the ticker \"SPY\": Set the risk as \"Baseline\"\n",
    "ticker_rv_aggregated.loc[ticker_rv_aggregated['Ticker'] == 'SPY', 'Risk'] = 'Baseline'\n",
    "\n",
    "# Sort by Risk\n",
    "ticker_rv_aggregated = ticker_rv_aggregated.sort_values('Risk')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(ticker_rv_aggregated)\n",
    "\n",
    "# Merge back to original data\n",
    "data = data.merge(ticker_rv_aggregated[['Ticker', 'Risk']], on='Ticker', how='left')\n",
    "\n",
    "# Just to be sure, make sure date is in datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "data.to_csv(\"../../data/processed_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
