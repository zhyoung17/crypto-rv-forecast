{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Raw file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "data_route = \"../../data/raw_data.csv\"\n",
    "data = pd.read_csv(data_route)\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of days of data for each ticker:\n",
      "Ticker\n",
      "BTC-USD     708\n",
      "DOGE-USD    708\n",
      "ETH-USD     708\n",
      "SOL-USD     708\n",
      "XRP-USD     708\n",
      "Name: Date, dtype: int64\n",
      "\n",
      "Daily data with number of transactions:\n",
      "    Ticker                      Date          Open          High  \\\n",
      "0  BTC-USD 2023-04-01 00:00:00+00:00  28473.332031  28802.457031   \n",
      "1  BTC-USD 2023-04-02 00:00:00+00:00  28462.845703  28518.958984   \n",
      "2  BTC-USD 2023-04-03 00:00:00+00:00  28183.080078  28475.623047   \n",
      "3  BTC-USD 2023-04-04 00:00:00+00:00  27795.273438  28433.742188   \n",
      "4  BTC-USD 2023-04-05 00:00:00+00:00  28169.726562  28739.238281   \n",
      "5  BTC-USD 2023-04-06 00:00:00+00:00  28175.226562  28178.384766   \n",
      "6  BTC-USD 2023-04-07 00:00:00+00:00  28038.966797  28111.593750   \n",
      "7  BTC-USD 2023-04-08 00:00:00+00:00  27923.943359  28159.863281   \n",
      "8  BTC-USD 2023-04-09 00:00:00+00:00  27952.367188  28532.830078   \n",
      "9  BTC-USD 2023-04-10 00:00:00+00:00  28336.027344  29771.464844   \n",
      "\n",
      "            Low         Close        Volume  Daily_Transactions  \n",
      "0  28297.171875  28466.199219  3.055288e+08                  24  \n",
      "1  27884.087891  28202.138672  2.249324e+09                  24  \n",
      "2  27276.720703  27786.367188  7.748741e+09                  24  \n",
      "3  27681.304688  28170.050781  1.185264e+09                  24  \n",
      "4  27843.763672  28175.435547  3.515677e+09                  24  \n",
      "5  27765.341797  28034.193359  9.813422e+08                  24  \n",
      "6  27794.031250  27925.455078  2.678426e+08                  24  \n",
      "7  27883.386719  27945.478516  4.694487e+08                  24  \n",
      "8  27828.480469  28348.171875  3.246046e+09                  24  \n",
      "9  28189.271484  29650.720703  7.995150e+09                  24  \n",
      "\n",
      "Average daily number of transactions for each ticker:\n",
      "Ticker\n",
      "BTC-USD     23.913842\n",
      "DOGE-USD    23.903955\n",
      "ETH-USD     23.912429\n",
      "SOL-USD     23.912429\n",
      "XRP-USD     23.913842\n",
      "Name: Daily_Transactions, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "daily_data = (\n",
    "    data.groupby(['Ticker', pd.Grouper(key='Date', freq='D')])  # Group by Ticker and Date (daily frequency)\n",
    "    .agg({\n",
    "        'Open': 'first',   # First price of the day\n",
    "        'High': 'max',     # Highest price of the day\n",
    "        'Low': 'min',      # Lowest price of the day\n",
    "        'Close': 'last',   # Last price of the day\n",
    "        'Volume': 'sum',   # Total volume of the day\n",
    "        'Ticker': 'size'   # Count the number of hourly data points (transactions)\n",
    "    })\n",
    "    .rename(columns={'Ticker': 'Daily_Transactions'})  # Rename the count column\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the total number of days of data for each ticker\n",
    "days_per_ticker = daily_data.groupby('Ticker')['Date'].nunique()\n",
    "\n",
    "# Display the results\n",
    "print(\"Number of days of data for each ticker:\")\n",
    "print(days_per_ticker)\n",
    "\n",
    "print(\"\\nDaily data with number of transactions:\")\n",
    "print(daily_data.head(10))\n",
    "\n",
    "# Average daily number of transactions\n",
    "avg_daily_transactions = daily_data.groupby('Ticker')['Daily_Transactions'].mean()\n",
    "print(\"\\nAverage daily number of transactions for each ticker:\")\n",
    "print(avg_daily_transactions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finding makes sense, as SPY does not operate over the weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 3 sets of data: Hourly, 3-hourly, and daily\n",
    "## Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Date is in datetime format and sort data by Ticker and Date\n",
    "hourly_data = data.copy()\n",
    "hourly_data['Date'] = pd.to_datetime(hourly_data['Date'])\n",
    "hourly_data = hourly_data.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# Compute **log hourly rv**\n",
    "hourly_data['ln_hourly_return'] = hourly_data.groupby('Ticker', group_keys=False)['Close'].transform(lambda x: np.log(x).diff())\n",
    "\n",
    "# Compute **hourly RV** (squared log returns)\n",
    "hourly_data['hourly_rv'] = hourly_data['ln_hourly_return'] ** 2\n",
    "\n",
    "# Ln the hourly RV\n",
    "hourly_data['ln_hourly_rv'] = np.log(hourly_data['hourly_rv'])\n",
    "\n",
    "# Lag the hourly RV\n",
    "hourly_data['ln_hourly_rv_lag1'] = hourly_data.groupby('Ticker')['ln_hourly_rv'].shift(1)\n",
    "hourly_data['ln_hourly_rv_lag2'] = hourly_data.groupby('Ticker')['ln_hourly_rv'].shift(2)\n",
    "hourly_data['ln_hourly_rv_lag3'] = hourly_data.groupby('Ticker')['ln_hourly_rv'].shift(3)\n",
    "\n",
    "# Lag RV by 8 and 24 for HAR model\n",
    "hourly_data['ln_hourly_rv_lag8'] = hourly_data.groupby('Ticker')['ln_hourly_rv'].shift(8)\n",
    "hourly_data['ln_hourly_rv_lag24'] = hourly_data.groupby('Ticker')['ln_hourly_rv'].shift(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_27788\\1506711666.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  three_hourly_data = three_hourly_data.groupby(['Ticker', pd.Grouper(key='Date', freq='3H')]).last().reset_index()\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_27788\\1506711666.py:21: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  raw.groupby(['Ticker', pd.Grouper(key='Date', freq='3H')])['hourly_rv']\n"
     ]
    }
   ],
   "source": [
    "# For 3-hourly data, aggregate to non-overlapping 3-hour blocks\n",
    "three_hourly_data = data.copy()\n",
    "three_hourly_data['Date'] = pd.to_datetime(three_hourly_data['Date'])\n",
    "three_hourly_data = three_hourly_data.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# Resample: get the last price of each 3-hour window\n",
    "three_hourly_data = three_hourly_data.groupby(['Ticker', pd.Grouper(key='Date', freq='3H')]).last().reset_index()\n",
    "\n",
    "# Compute log 3-hourly return (compare current price to price 3 hours ago)\n",
    "three_hourly_data['ln_3_hourly_return'] = three_hourly_data.groupby('Ticker')['Close'].transform(lambda x: np.log(x).diff())\n",
    "\n",
    "# For 3-hourly RV, sum the hourly RV over each 3-hour window.\n",
    "# First, compute hourly RV on raw data.\n",
    "raw = data.copy()\n",
    "raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "raw = raw.sort_values(['Ticker', 'Date'])\n",
    "raw['ln_hourly_return'] = raw.groupby('Ticker')['Close'].transform(lambda x: np.log(x).diff())\n",
    "raw['hourly_rv'] = raw['ln_hourly_return'] ** 2\n",
    "\n",
    "three_hourly_rv = (\n",
    "    raw.groupby(['Ticker', pd.Grouper(key='Date', freq='3H')])['hourly_rv']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'hourly_rv': '3_hourly_rv'})\n",
    ")\n",
    "# Merge the 3-hourly RV into the three_hourly_data DataFrame\n",
    "three_hourly_data = three_hourly_data.merge(three_hourly_rv, on=['Ticker', 'Date'], how='left')\n",
    "three_hourly_data['ln_3_hourly_rv'] = np.log(three_hourly_data['3_hourly_rv']).replace(-np.inf, np.nan)\n",
    "\n",
    "# Lag the 3-hourly RV\n",
    "three_hourly_data['ln_3_hourly_rv_lag1'] = three_hourly_data.groupby('Ticker')['ln_3_hourly_rv'].shift(1)\n",
    "three_hourly_data['ln_3_hourly_rv_lag2'] = three_hourly_data.groupby('Ticker')['ln_3_hourly_rv'].shift(2)\n",
    "three_hourly_data['ln_3_hourly_rv_lag3'] = three_hourly_data.groupby('Ticker')['ln_3_hourly_rv'].shift(3)\n",
    "\n",
    "# Lag RV by 4 and 8 for HAR model: This represents a 12 hour and 24 hour lag\n",
    "three_hourly_data['ln_3_hourly_rv_lag4'] = three_hourly_data.groupby('Ticker')['ln_3_hourly_rv'].shift(4)\n",
    "three_hourly_data['ln_3_hourly_rv_lag8'] = three_hourly_data.groupby('Ticker')['ln_3_hourly_rv'].shift(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "daily_data = data.copy()\n",
    "daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "daily_data = daily_data.sort_values(['Ticker', 'Date'])\n",
    "\n",
    "# Resample: get the last price of each day (or use a method that suits your analysis)\n",
    "daily_data = daily_data.groupby(['Ticker', pd.Grouper(key='Date', freq='D')]).last().reset_index()\n",
    "\n",
    "# Compute daily RV by summing the hourly RVs\n",
    "# Recompute hourly RV on raw data if necessary\n",
    "raw['Date'] = pd.to_datetime(raw['Date'])\n",
    "raw = raw.sort_values(['Ticker', 'Date'])\n",
    "raw['ln_hourly_return'] = raw.groupby('Ticker')['Close'].transform(lambda x: np.log(x).diff())\n",
    "raw['hourly_rv'] = raw['ln_hourly_return'] ** 2\n",
    "\n",
    "daily_rv = (\n",
    "    raw.groupby(['Ticker', pd.Grouper(key='Date', freq='D')])['hourly_rv']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={'hourly_rv': 'daily_rv'})\n",
    ")\n",
    "daily_data = daily_data.merge(daily_rv, on=['Ticker', 'Date'], how='left')\n",
    "daily_data['ln_daily_rv'] = np.log(daily_data['daily_rv']).replace(-np.inf, np.nan)\n",
    "\n",
    "# Compute daily returns from the daily closing prices\n",
    "daily_data['daily_return'] = daily_data.groupby('Ticker')['Close'].pct_change()\n",
    "daily_data['ln_daily_return'] = np.log(daily_data['Close'] / daily_data.groupby('Ticker')['Close'].shift(1))\n",
    "\n",
    "# Lag the daily RV\n",
    "daily_data['ln_daily_rv_lag1'] = daily_data.groupby('Ticker')['ln_daily_rv'].shift(1)\n",
    "daily_data['ln_daily_rv_lag2'] = daily_data.groupby('Ticker')['ln_daily_rv'].shift(2)\n",
    "daily_data['ln_daily_rv_lag3'] = daily_data.groupby('Ticker')['ln_daily_rv'].shift(3)\n",
    "\n",
    "# Calculate the weekly rv: Sum of past 7 days, and then divide by 7\n",
    "daily_data['weekly_rv'] = daily_data.groupby('Ticker')['daily_rv'].rolling(window=7).sum().reset_index(0, drop=True) / 7\n",
    "daily_data['ln_weekly_rv'] = np.log(daily_data['weekly_rv']).replace(-np.inf, np.nan)\n",
    "daily_data['ln_weekly_rv_lag1'] = daily_data.groupby('Ticker')['ln_weekly_rv'].shift(1)\n",
    "\n",
    "# Calculate the monthly rv: Sum of past 30 days, and then divide by 30\n",
    "daily_data['monthly_rv'] = daily_data.groupby('Ticker')['daily_rv'].rolling(window=30).sum().reset_index(0, drop=True) / 30\n",
    "daily_data['ln_monthly_rv'] = np.log(daily_data['monthly_rv']).replace(-np.inf, np.nan)\n",
    "daily_data['ln_monthly_rv_lag1'] = daily_data.groupby('Ticker')['ln_monthly_rv'].shift(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "In my model, I wish to classify each coin as either high risk, medium risk, or low risk, based on their hourly realised variance. High RV constitutes as high risk, and likewise for medium risk and low risk. I've used a weighting system to determine the risk level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the training data to do the aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-01 00:00:00+00:00 2024-04-01 00:00:00+00:00\n",
      "2024-04-01 01:00:00+00:00 2025-03-09 23:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Make date column datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Base the eda on the training data\n",
    "data = data.sort_values('Date')\n",
    "\n",
    "# Determine when the first year ends, and use it as train data\n",
    "# The rest of the data is used as test data\n",
    "min_date = data['Date'].min()\n",
    "max_date = data['Date'].max()\n",
    "\n",
    "# Calculate the total time span of the data\n",
    "total_time_span = max_date - min_date\n",
    "\n",
    "# Define the first year of data\n",
    "first_year_end = min_date + pd.DateOffset(years=1)\n",
    "\n",
    "# Filter data for the first year\n",
    "first_year_data = data[data['Date'] <= first_year_end]\n",
    "\n",
    "# Calculate the percentage of data in the first year\n",
    "percentage_first_year = (len(first_year_data) / len(data))\n",
    "\n",
    "train_split = percentage_first_year\n",
    "train_data = data[:int(train_split * len(data))]\n",
    "test_data = data[int(train_split * len(data)):]\n",
    "\n",
    "# Print train and test data date\n",
    "print(train_data['Date'].min(), train_data['Date'].max())\n",
    "print(test_data['Date'].min(), test_data['Date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Classification:\n",
      "     Ticker  mean_ln_hourly_rv  mean_ln_3_hourly_rv  mean_ln_daily_rv  \\\n",
      "0   BTC-USD         -12.999954           -10.951066         -8.212067   \n",
      "2   ETH-USD         -12.789053           -10.688543         -7.948952   \n",
      "4   XRP-USD         -12.265120           -10.170671         -7.456567   \n",
      "1  DOGE-USD         -12.126309           -10.025884         -7.292734   \n",
      "3   SOL-USD         -11.200828            -9.144045         -6.520270   \n",
      "\n",
      "   Composite_Score         Risk  \n",
      "0        -9.991344     Low Risk  \n",
      "2        -9.738849     Low Risk  \n",
      "4        -9.232509  Medium Risk  \n",
      "1        -9.079394    High Risk  \n",
      "3        -8.243514    High Risk  \n",
      "3-hourly data with risk classification:\n",
      "Index(['Ticker', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
      "       'ln_3_hourly_return', '3_hourly_rv', 'ln_3_hourly_rv',\n",
      "       'ln_3_hourly_rv_lag1', 'ln_3_hourly_rv_lag2', 'ln_3_hourly_rv_lag3',\n",
      "       'ln_3_hourly_rv_lag4', 'ln_3_hourly_rv_lag8', 'Risk'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Filter training data by date (for hourly, 3-hourly, and daily data)\n",
    "hourly_data_train = hourly_data[hourly_data['Date'] <= first_year_end]\n",
    "three_hourly_data_train = three_hourly_data[three_hourly_data['Date'] <= first_year_end]\n",
    "daily_data_train = daily_data[daily_data['Date'] <= first_year_end]\n",
    "\n",
    "# Compute mean log returns for each ticker\n",
    "hourly_data_mean = hourly_data_train.groupby('Ticker')['ln_hourly_rv'].mean()\n",
    "three_hourly_data_mean = three_hourly_data_train.groupby('Ticker')['ln_3_hourly_rv'].mean()\n",
    "daily_data_mean = daily_data_train.groupby('Ticker')['ln_daily_rv'].mean()\n",
    "\n",
    "risk_df = pd.concat([hourly_data_mean, three_hourly_data_mean, daily_data_mean], axis=1).reset_index()\n",
    "risk_df.columns = ['Ticker', 'mean_ln_hourly_rv', 'mean_ln_3_hourly_rv', 'mean_ln_daily_rv']\n",
    "\n",
    "# Define weights: e.g., giving more weight to the daily return since it's more stable.\n",
    "weights = {\n",
    "    'mean_ln_hourly_rv': 0.2,\n",
    "    'mean_ln_3_hourly_rv': 0.3,\n",
    "    'mean_ln_daily_rv': 0.5\n",
    "}\n",
    "\n",
    "risk_df['Composite_Score'] = (\n",
    "    risk_df['mean_ln_hourly_rv'] * weights['mean_ln_hourly_rv'] +\n",
    "    risk_df['mean_ln_3_hourly_rv'] * weights['mean_ln_3_hourly_rv'] +\n",
    "    risk_df['mean_ln_daily_rv'] * weights['mean_ln_daily_rv']\n",
    ")\n",
    "\n",
    "# --- Define Risk Bins and Classify ---\n",
    "# Create bins using the 33rd and 66th percentiles as thresholds.\n",
    "bins = [\n",
    "    risk_df['Composite_Score'].min(),\n",
    "    risk_df['Composite_Score'].quantile(0.33),\n",
    "    risk_df['Composite_Score'].quantile(0.66),\n",
    "    risk_df['Composite_Score'].max()\n",
    "]\n",
    "bins = sorted(list(set(bins)))  # Remove duplicates if any and sort\n",
    "\n",
    "# Define risk labels\n",
    "risk_labels = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "\n",
    "# Classify tickers into risk groups based on the composite score\n",
    "risk_df['Risk'] = pd.cut(\n",
    "    risk_df['Composite_Score'],\n",
    "    bins=bins,\n",
    "    labels=risk_labels,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Sort\n",
    "risk_df = risk_df.sort_values('Composite_Score')\n",
    "print(\"Risk Classification:\")\n",
    "print(risk_df)\n",
    "risk_df.to_csv('../../data/risk_classification.csv', index=False)\n",
    "\n",
    "hourly_data = hourly_data.merge(risk_df[['Ticker', 'Risk']], on='Ticker', how='left')\n",
    "three_hourly_data = three_hourly_data.merge(risk_df[['Ticker', 'Risk']], on='Ticker', how='left')\n",
    "daily_data = daily_data.merge(risk_df[['Ticker', 'Risk']], on='Ticker', how='left')\n",
    "\n",
    "print(\"3-hourly data with risk classification:\")\n",
    "print(three_hourly_data.columns)\n",
    "\n",
    "\n",
    "# Save the data\n",
    "hourly_data.to_csv('../../data/hourly_data.csv', index=False)\n",
    "three_hourly_data.to_csv('../../data/three_hourly_data.csv', index=False)\n",
    "daily_data.to_csv('../../data/daily_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
