{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing 51/49 split to 80/20 split\n",
    "In theory, an 80/20 split might be ideal to get a balanced train-test split. But given the volatile nature of cryptocurrency, it might be ideal to dedicate less data to training and more to test to prevent overfitting.\n",
    "\n",
    "This paper uses the EWMA as the baseline, and will be using it to test the data splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51/49 Split\n",
    "This paper mainly employed a 51/49 split, so it will be calling the results directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Frequency",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Risk Group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Ticker",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "MSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R²",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0eef28d8-1494-4f4f-959e-a9e51053d393",
       "rows": [
        [
         "0",
         "hourly",
         "Low",
         "BTC-USD",
         "6.100476145958836",
         "0.0379744109764036"
        ],
        [
         "1",
         "hourly",
         "Low",
         "ETH-USD",
         "5.934582971468532",
         "0.0314363870958425"
        ],
        [
         "2",
         "hourly",
         "Medium",
         "XRP-USD",
         "6.050096291555469",
         "0.1374188863572468"
        ],
        [
         "3",
         "hourly",
         "High",
         "DOGE-USD",
         "6.524613660853849",
         "-0.0442277366468804"
        ],
        [
         "4",
         "hourly",
         "High",
         "SOL-USD",
         "6.511406369727041",
         "-0.0776801652918262"
        ],
        [
         "5",
         "3hourly",
         "Low",
         "BTC-USD",
         "2.157549328682435",
         "0.0874559135465324"
        ],
        [
         "6",
         "3hourly",
         "Low",
         "ETH-USD",
         "1.8897166146842157",
         "0.1230178328650756"
        ],
        [
         "7",
         "3hourly",
         "Medium",
         "XRP-USD",
         "1.90047203240755",
         "0.3689389066900352"
        ],
        [
         "8",
         "3hourly",
         "High",
         "DOGE-USD",
         "1.7710564613915885",
         "0.1866822617094864"
        ],
        [
         "9",
         "3hourly",
         "High",
         "SOL-USD",
         "1.6005618662116194",
         "0.1268838838369839"
        ],
        [
         "10",
         "daily",
         "Low",
         "BTC-USD",
         "1.0869076705670448",
         "-0.0316554111815257"
        ],
        [
         "11",
         "daily",
         "Low",
         "ETH-USD",
         "0.8575320710266046",
         "0.0684776572895948"
        ],
        [
         "12",
         "daily",
         "Medium",
         "XRP-USD",
         "1.3105793994106985",
         "0.3251968097624708"
        ],
        [
         "13",
         "daily",
         "High",
         "DOGE-USD",
         "0.8515029695331056",
         "0.1167119072895629"
        ],
        [
         "14",
         "daily",
         "High",
         "SOL-USD",
         "0.7159907993399738",
         "0.035647354501674"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Risk Group</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>MSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hourly</td>\n",
       "      <td>Low</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>6.100476</td>\n",
       "      <td>0.037974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hourly</td>\n",
       "      <td>Low</td>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>5.934583</td>\n",
       "      <td>0.031436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hourly</td>\n",
       "      <td>Medium</td>\n",
       "      <td>XRP-USD</td>\n",
       "      <td>6.050096</td>\n",
       "      <td>0.137419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hourly</td>\n",
       "      <td>High</td>\n",
       "      <td>DOGE-USD</td>\n",
       "      <td>6.524614</td>\n",
       "      <td>-0.044228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hourly</td>\n",
       "      <td>High</td>\n",
       "      <td>SOL-USD</td>\n",
       "      <td>6.511406</td>\n",
       "      <td>-0.077680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>Low</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>2.157549</td>\n",
       "      <td>0.087456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>Low</td>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>1.889717</td>\n",
       "      <td>0.123018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>Medium</td>\n",
       "      <td>XRP-USD</td>\n",
       "      <td>1.900472</td>\n",
       "      <td>0.368939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>High</td>\n",
       "      <td>DOGE-USD</td>\n",
       "      <td>1.771056</td>\n",
       "      <td>0.186682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>High</td>\n",
       "      <td>SOL-USD</td>\n",
       "      <td>1.600562</td>\n",
       "      <td>0.126884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>daily</td>\n",
       "      <td>Low</td>\n",
       "      <td>BTC-USD</td>\n",
       "      <td>1.086908</td>\n",
       "      <td>-0.031655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>daily</td>\n",
       "      <td>Low</td>\n",
       "      <td>ETH-USD</td>\n",
       "      <td>0.857532</td>\n",
       "      <td>0.068478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>daily</td>\n",
       "      <td>Medium</td>\n",
       "      <td>XRP-USD</td>\n",
       "      <td>1.310579</td>\n",
       "      <td>0.325197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>daily</td>\n",
       "      <td>High</td>\n",
       "      <td>DOGE-USD</td>\n",
       "      <td>0.851503</td>\n",
       "      <td>0.116712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>daily</td>\n",
       "      <td>High</td>\n",
       "      <td>SOL-USD</td>\n",
       "      <td>0.715991</td>\n",
       "      <td>0.035647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Frequency Risk Group    Ticker       MSE        R²\n",
       "0     hourly        Low   BTC-USD  6.100476  0.037974\n",
       "1     hourly        Low   ETH-USD  5.934583  0.031436\n",
       "2     hourly     Medium   XRP-USD  6.050096  0.137419\n",
       "3     hourly       High  DOGE-USD  6.524614 -0.044228\n",
       "4     hourly       High   SOL-USD  6.511406 -0.077680\n",
       "5    3hourly        Low   BTC-USD  2.157549  0.087456\n",
       "6    3hourly        Low   ETH-USD  1.889717  0.123018\n",
       "7    3hourly     Medium   XRP-USD  1.900472  0.368939\n",
       "8    3hourly       High  DOGE-USD  1.771056  0.186682\n",
       "9    3hourly       High   SOL-USD  1.600562  0.126884\n",
       "10     daily        Low   BTC-USD  1.086908 -0.031655\n",
       "11     daily        Low   ETH-USD  0.857532  0.068478\n",
       "12     daily     Medium   XRP-USD  1.310579  0.325197\n",
       "13     daily       High  DOGE-USD  0.851503  0.116712\n",
       "14     daily       High   SOL-USD  0.715991  0.035647"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Load the data\n",
    "main_split_results = pd.read_csv('../../results/appendix/ewma_summary.csv')\n",
    "main_split_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80/20 Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67716 16930\n",
      "22582 5646\n",
      "2832 708\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "hourly_data = pd.read_csv('../../data/hourly_data.csv')\n",
    "three_hourly_data = pd.read_csv('../../data/three_hourly_data.csv')\n",
    "daily_data = pd.read_csv('../../data/daily_data.csv')\n",
    "\n",
    "# Sort the data by date\n",
    "hourly_data['Date'] = pd.to_datetime(hourly_data['Date'])\n",
    "three_hourly_data['Date'] = pd.to_datetime(three_hourly_data['Date'])\n",
    "daily_data['Date'] = pd.to_datetime(daily_data['Date'])\n",
    "\n",
    "hourly_data = hourly_data.sort_values('Date')\n",
    "\n",
    "train_split = 0.8\n",
    "test_split = 0.2\n",
    "\n",
    "# Split the data\n",
    "X_train_hourly = hourly_data.iloc[:int(len(hourly_data)*train_split)]\n",
    "X_test_hourly = hourly_data.iloc[int(len(hourly_data)*train_split):]\n",
    "\n",
    "X_train_three_hourly = three_hourly_data.iloc[:int(len(three_hourly_data)*train_split)]\n",
    "X_test_three_hourly = three_hourly_data.iloc[int(len(three_hourly_data)*train_split):]\n",
    "\n",
    "X_train_daily = daily_data.iloc[:int(len(daily_data)*train_split)]\n",
    "X_test_daily = daily_data.iloc[int(len(daily_data)*train_split):]\n",
    "\n",
    "X_train_hourly\n",
    "\n",
    "print(len(X_train_hourly), len(X_test_hourly))\n",
    "print(len(X_train_three_hourly), len(X_test_three_hourly))\n",
    "print(len(X_train_daily), len(X_test_daily))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further split the data based on the risk level\n",
    "There are low, medium, and high risk models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date_split_hourly = {\n",
    "    'low': X_train_hourly[X_train_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_hourly[X_train_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_hourly[X_train_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_hourly = {\n",
    "    'low': X_test_hourly[X_test_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_hourly[X_test_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_hourly[X_test_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "train_date_split_three_hourly = {\n",
    "    'low': X_train_three_hourly[X_train_three_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_three_hourly[X_train_three_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_three_hourly[X_train_three_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_three_hourly = {\n",
    "    'low': X_test_three_hourly[X_test_three_hourly['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_three_hourly[X_test_three_hourly['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_three_hourly[X_test_three_hourly['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "train_date_split_daily = {\n",
    "    'low': X_train_daily[X_train_daily['Risk'] == 'Low Risk'],\n",
    "    'medium': X_train_daily[X_train_daily['Risk'] == 'Medium Risk'],\n",
    "    'high': X_train_daily[X_train_daily['Risk'] == 'High Risk']\n",
    "}\n",
    "\n",
    "test_date_split_daily = {\n",
    "    'low': X_test_daily[X_test_daily['Risk'] == 'Low Risk'],\n",
    "    'medium': X_test_daily[X_test_daily['Risk'] == 'Medium Risk'],\n",
    "    'high': X_test_daily[X_test_daily['Risk'] == 'High Risk']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model based on their classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecast function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ewma_forecast(series, lam):\n",
    "    # Initialize the forecast with the first observed value\n",
    "    forecasts = [series.iloc[0]]\n",
    "    for i in range(1, len(series)):\n",
    "        f = lam * forecasts[i-1] + (1 - lam) * series.iloc[i-1]\n",
    "        forecasts.append(f)\n",
    "    return np.array(forecasts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating optimal lambda\n",
    "In theory, optimal lambda is 0.94, but given the abundance of training data, we can use gridsearch to find optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lambda(series, candidate_lambdas=np.linspace(0.90, 0.99, 100)):\n",
    "    best_lambda = None\n",
    "    best_mse = np.inf\n",
    "    for lam in candidate_lambdas:\n",
    "        forecast = compute_ewma_forecast(series, lam)\n",
    "        # Exclude the first forecast because it is just the initialization\n",
    "        mse = np.mean((series.values[1:] - forecast[1:])**2)\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_lambda = lam\n",
    "    return best_lambda, best_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wil give us 3 models to work with: model_low, model_medium, and model_high. We will use these subsequent models on the test data to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_22496\\1872742783.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, pd.DataFrame([summary_row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insufficient data for daily-medium, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_22496\\1872742783.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  model_summary = pd.concat([model_summary, pd.DataFrame([summary_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to train models and summarize them\n",
    "def train_models_by_frequency_and_risk(train_data, frequencies):\n",
    "    models = {}\n",
    "    model_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Optimal Lambda', 'In-Sample MSE'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        models[freq] = {}\n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            group_data = train_data[risk_group]\n",
    "            \n",
    "            # Select target based on frequency\n",
    "            if freq == 'hourly':\n",
    "                target = 'ln_hourly_rv'\n",
    "            elif freq == '3hourly':\n",
    "                target = 'ln_3_hourly_rv'\n",
    "            elif freq == 'daily':\n",
    "                target = 'ln_daily_rv'\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported frequency: {freq}\")\n",
    "            \n",
    "            if target not in group_data.columns:\n",
    "                print(f\"Missing {target} for {freq}-{risk_group}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Drop missing values\n",
    "            series = group_data[target].dropna()\n",
    "            if len(series) < 2:\n",
    "                print(f\"Insufficient data for {freq}-{risk_group}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Optimize lambda for the EWMA model\n",
    "            optimal_lambda, in_sample_mse = optimize_lambda(series)\n",
    "            \n",
    "            # Store the \"trained\" model: here just the optimal lambda and initial value\n",
    "            ewma_model = {'lambda': optimal_lambda, 'initial': series.iloc[0]}\n",
    "            models[freq][risk_group] = ewma_model\n",
    "            \n",
    "            # Append results to the summary DataFrame\n",
    "            summary_row = {\n",
    "                'Frequency': freq,\n",
    "                'Risk Group': risk_group,\n",
    "                'Optimal Lambda': optimal_lambda,\n",
    "                'In-Sample MSE': in_sample_mse\n",
    "            }\n",
    "            model_summary = pd.concat([model_summary, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "    \n",
    "    return models, model_summary\n",
    "\n",
    "model_daily, summary_daily = train_models_by_frequency_and_risk(train_date_split_daily, ['daily'])\n",
    "model_hourly, summary_hourly = train_models_by_frequency_and_risk(train_date_split_hourly, ['hourly'])\n",
    "model_three_hourly, summary_three_hourly = train_models_by_frequency_and_risk(train_date_split_three_hourly, ['3hourly'])\n",
    "\n",
    "# Merge the summaries\n",
    "summary = pd.concat([summary_daily, summary_hourly, summary_three_hourly], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Frequency",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Risk Group",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Optimal Lambda",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "In-Sample MSE",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "05e83c90-d9cf-4c38-a749-e4c57e2e1ad2",
       "rows": [
        [
         "0",
         "daily",
         "low",
         "0.9",
         "0.975163228479765"
        ],
        [
         "1",
         "daily",
         "high",
         "0.9",
         "0.8476269619472682"
        ],
        [
         "2",
         "hourly",
         "low",
         "0.9009090909090909",
         "5.813577269807388"
        ],
        [
         "3",
         "hourly",
         "medium",
         "0.9381818181818182",
         "5.949516149336619"
        ],
        [
         "4",
         "hourly",
         "high",
         "0.9572727272727273",
         "6.051113149094096"
        ],
        [
         "5",
         "3hourly",
         "low",
         "0.9",
         "2.0082403768351664"
        ],
        [
         "6",
         "3hourly",
         "high",
         "0.9",
         "1.7255621226574178"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 7
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Risk Group</th>\n",
       "      <th>Optimal Lambda</th>\n",
       "      <th>In-Sample MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daily</td>\n",
       "      <td>low</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.975163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily</td>\n",
       "      <td>high</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.847627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hourly</td>\n",
       "      <td>low</td>\n",
       "      <td>0.900909</td>\n",
       "      <td>5.813577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hourly</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.938182</td>\n",
       "      <td>5.949516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hourly</td>\n",
       "      <td>high</td>\n",
       "      <td>0.957273</td>\n",
       "      <td>6.051113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>low</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>2.008240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3hourly</td>\n",
       "      <td>high</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.725562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Frequency Risk Group  Optimal Lambda  In-Sample MSE\n",
       "0     daily        low        0.900000       0.975163\n",
       "1     daily       high        0.900000       0.847627\n",
       "2    hourly        low        0.900909       5.813577\n",
       "3    hourly     medium        0.938182       5.949516\n",
       "4    hourly       high        0.957273       6.051113\n",
       "5   3hourly        low        0.900000       2.008240\n",
       "6   3hourly       high        0.900000       1.725562"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing rolling window\n",
    "Rolling window is used for a one step ahead forecast. So we constantly update the lagged data with an update lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_predictions(X_test, y_test, model, window_size=24, step_ahead=1):\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    lam = model['lambda']  # Get the decay parameter from the trained EWMA model\n",
    "    \n",
    "    max_index = len(X_test) - step_ahead  # Ensure we have data for the forecast horizon\n",
    "    for i in range(window_size, max_index + 1):\n",
    "        # Use the target series within the current rolling window\n",
    "        window_data = y_test.iloc[i - window_size : i]\n",
    "        \n",
    "        # Compute the EWMA forecast using the provided decay factor\n",
    "        forecast = compute_ewma_forecast(window_data, lam)\n",
    "        # For EWMA, the one-step ahead forecast is the last computed value.\n",
    "        y_pred = forecast[-1]\n",
    "        # For multi-step ahead, under basic EWMA assumptions, the forecast remains constant.\n",
    "        \n",
    "        # Get the actual value at the forecast time point\n",
    "        actual_index = i + step_ahead - 1\n",
    "        actual_value = y_test.iloc[actual_index]\n",
    "        \n",
    "        # Capture the corresponding date from the X_test 'Date' column\n",
    "        current_date = X_test['Date'].iloc[actual_index]\n",
    "        \n",
    "        predictions.append(y_pred)\n",
    "        actuals.append(actual_value)\n",
    "        dates.append(current_date)\n",
    "    \n",
    "    return predictions, actuals, dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing an evaluatation function\n",
    "This function evaluates the findings and puts it in a df for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_on_test_data(test_data_split, models, frequencies, window_size=24):\n",
    "    evaluation_summary = pd.DataFrame(columns=['Frequency', 'Risk Group', 'Ticker', 'MSE', 'R²'])\n",
    "    detailed_results = pd.DataFrame(columns=['Date', 'Ticker', 'Risk Group', 'Frequency', 'Predicted', 'Actual'])\n",
    "    \n",
    "    for freq in frequencies:\n",
    "        for risk_group in ['low', 'medium', 'high']:\n",
    "            model = models[freq][risk_group]\n",
    "            group_data = test_data_split[risk_group].copy()\n",
    "            \n",
    "            # Define target and forecast horizon based on frequency\n",
    "            if freq == 'hourly':\n",
    "                target = 'ln_hourly_rv'\n",
    "                step_ahead = 1  # Forecast 1 step ahead\n",
    "            elif freq == '3hourly':\n",
    "                target = 'ln_3_hourly_rv'\n",
    "                step_ahead = 1  # Forecast 3 steps ahead\n",
    "            elif freq == 'daily':\n",
    "                target = 'ln_daily_rv'\n",
    "                step_ahead = 1  # Forecast 24 steps ahead (1 day)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported frequency\")\n",
    "            \n",
    "            unique_tickers = group_data['Ticker'].unique()\n",
    "            \n",
    "            for ticker in unique_tickers:\n",
    "                ticker_data = group_data[group_data['Ticker'] == ticker].copy()\n",
    "                \n",
    "                # Validate that the required columns exist\n",
    "                if 'Date' not in ticker_data.columns or target not in ticker_data.columns:\n",
    "                    print(f\"Skipping {ticker}: missing Date or {target} for {freq}-{risk_group}\")\n",
    "                    continue\n",
    "                \n",
    "                # Prepare test data with Date and target column\n",
    "                X_test = ticker_data[['Date', target]].dropna()\n",
    "                y_test = X_test[target]\n",
    "                \n",
    "                # Check for sufficient data\n",
    "                if len(X_test) < window_size + step_ahead:\n",
    "                    print(f\"Skipping {ticker}: insufficient data ({len(X_test)} rows)\")\n",
    "                    continue\n",
    "                \n",
    "                # Get EWMA-based predictions via a rolling window\n",
    "                predictions, actuals, dates = rolling_window_predictions(\n",
    "                    X_test, y_test, model, window_size=window_size, step_ahead=step_ahead\n",
    "                )\n",
    "                \n",
    "                if len(predictions) == 0:\n",
    "                    continue\n",
    "                                \n",
    "                # Calculate metrics on the log-transformed values\n",
    "                mse = mean_squared_error(actuals, predictions)\n",
    "                r2 = r2_score(actuals, predictions)\n",
    "                \n",
    "                eval_row = {\n",
    "                    'Frequency': freq,\n",
    "                    'Risk Group': risk_group.capitalize(),\n",
    "                    'Ticker': ticker,\n",
    "                    'MSE': mse,\n",
    "                    'R²': r2\n",
    "                }\n",
    "                evaluation_summary = pd.concat(\n",
    "                    [evaluation_summary, pd.DataFrame([eval_row])], ignore_index=True\n",
    "                )\n",
    "                \n",
    "                ticker_results = pd.DataFrame({\n",
    "                    'Date': dates,\n",
    "                    'Ticker': ticker,\n",
    "                    'Risk Group': risk_group.capitalize(),\n",
    "                    'Frequency': freq,\n",
    "                    'Predicted': predictions,\n",
    "                    'Actual': actuals\n",
    "                })\n",
    "                detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n",
    "    \n",
    "    return evaluation_summary, detailed_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_22496\\2207126206.py:61: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  evaluation_summary = pd.concat(\n",
      "C:\\Users\\young\\AppData\\Local\\Temp\\ipykernel_22496\\2207126206.py:73: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  detailed_results = pd.concat([detailed_results, ticker_results], ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'medium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate models on test data for each frequency.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m evaluation_summary_hourly, detailed_results_hourly \u001b[38;5;241m=\u001b[39m evaluate_models_on_test_data(\n\u001b[0;32m      6\u001b[0m     test_date_split_hourly, model_hourly, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhourly\u001b[39m\u001b[38;5;124m'\u001b[39m], window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m evaluation_summary_three_hourly, detailed_results_three_hourly \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_models_on_test_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_date_split_three_hourly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_three_hourly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m3hourly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m evaluation_summary_daily, detailed_results_daily \u001b[38;5;241m=\u001b[39m evaluate_models_on_test_data(\n\u001b[0;32m     12\u001b[0m     test_date_split_daily, model_daily, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaily\u001b[39m\u001b[38;5;124m'\u001b[39m], window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Combine the evaluation summaries across frequencies, if desired.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mevaluate_models_on_test_data\u001b[1;34m(test_data_split, models, frequencies, window_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m freq \u001b[38;5;129;01min\u001b[39;00m frequencies:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m risk_group \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 7\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrisk_group\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m         group_data \u001b[38;5;241m=\u001b[39m test_data_split[risk_group]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Define target and forecast horizon based on frequency\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'medium'"
     ]
    }
   ],
   "source": [
    "# Define frequencies\n",
    "frequencies = ['hourly', '3hourly', 'daily']\n",
    "\n",
    "# Evaluate models on test data for each frequency.\n",
    "evaluation_summary_hourly, detailed_results_hourly = evaluate_models_on_test_data(\n",
    "    test_date_split_hourly, model_hourly, ['hourly'], window_size=24\n",
    ")\n",
    "evaluation_summary_three_hourly, detailed_results_three_hourly = evaluate_models_on_test_data(\n",
    "    test_date_split_three_hourly, model_three_hourly, ['3hourly'], window_size=24\n",
    ")\n",
    "evaluation_summary_daily, detailed_results_daily = evaluate_models_on_test_data(\n",
    "    test_date_split_daily, model_daily, ['daily'], window_size=24\n",
    ")\n",
    "\n",
    "# Combine the evaluation summaries across frequencies, if desired.\n",
    "combined_summary = pd.concat([evaluation_summary_hourly, evaluation_summary_three_hourly, evaluation_summary_daily], ignore_index=True)\n",
    "combined_details = pd.concat([detailed_results_hourly, detailed_results_three_hourly, detailed_results_daily], ignore_index=True)\n",
    "\n",
    "# Save the detailed results to CSV\n",
    "combined_details.to_csv('../../results/appendix/large_split_ewma.csv', index=False)\n",
    "\n",
    "# Print the evaluation summaries\n",
    "print(\"Combined Evaluation Summary:\")\n",
    "print(combined_summary)\n",
    "\n",
    "combined_summary.to_csv('../../results/appendix/large_split_ewma_summary.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
